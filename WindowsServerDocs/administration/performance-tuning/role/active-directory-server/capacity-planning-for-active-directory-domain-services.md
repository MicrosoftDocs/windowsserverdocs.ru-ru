---
title: Планирование ресурсов для служб домен Active Directory
description: Подробное обсуждение факторов, которые следует учитывать при планировании загрузки AD DS.
ms.prod: windows-server-threshold
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: v-tea; kenbrunf
author: Teresa-Motiv
ms.date: 7/3/2019
ms.openlocfilehash: dac13ac94e38cf671239d35507e07d7ac3a0c1ab
ms.sourcegitcommit: f6490192d686f0a1e0c2ebe471f98e30105c0844
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/10/2019
ms.locfileid: "70866727"
---
# <a name="capacity-planning-for-active-directory-domain-services"></a>Планирование ресурсов для служб домен Active Directory

Эта статья первоначально написана Кен Брумфиелд, старшим инженером по выезду на уровне Premier в корпорации Майкрософт и содержит рекомендации по планированию ресурсов для служб домен Active Directory Services (AD DS).

## <a name="goals-of-capacity-planning"></a>Цели планирования ресурсов

Планирование ресурсов не так же, как устранение проблем с производительностью. Они тесно связаны, но сильно отличаются друг от друга. Цели планирования ресурсов:  

- Правильная реализация и эксплуатация среды 
- Сократите время, затрачиваемое на устранение проблем с производительностью.  
  
При планировании емкости организация может иметь базовую цель загрузки процессора 40% в периоды пиковой нагрузки, чтобы удовлетворить требования к производительности клиента и обеспечить время, необходимое для обновления оборудования в центре обработки данных. В то же время, чтобы получать уведомления о аномальных инцидентах производительности, порог оповещения мониторинга может быть установлен в 90% за 5-минутный интервал.

Разница заключается в том, что при постоянном превышении порогового значения управления емкостью (одноразовое событие не является проблемой) Добавление емкости (т. е. Добавление в более или более быстрых процессорах) является решением или масштабированием службы по нескольким серверам. решение. Пороговые значения предупреждений производительности указывают на то, что в настоящее время взаимодействие с клиентом низкий, и для решения этой проблемы требуются немедленные действия.

В качестве аналога: Управление емкостью заключается в предотвращении случайного аварии машины (защитная работа, обеспечение нормальной работы Brakes и т. д.), в то время как устранение неполадок производительности — это то, что делает политика, пожарный отдел и экстренные специалисты. После аварии. Это "Защитная" Active Directory ".

За последние несколько лет руководство по планированию загрузки для систем масштабирования существенно изменилось. Следующие изменения в системных архитектурах имеют некоторые фундаментальные предположения о проектировании и масштабировании службы.

- 64-разрядные серверные платформы  
- Виртуализация  
- Повышенное внимание на потребляемую мощность  
- Хранилище SSD  
- Облачные сценарии  

Кроме того, подход заключается в том, что в процессе планирования емкости на основе сервера в качестве подхода к планированию загрузки на уровне службы используется метод. Домен Active Directory Services (AD DS) — это преждевременно распределенная служба, которую многие продукты корпорации Майкрософт и сторонних производителей используют в качестве серверной части.

### <a name="baseline-requirements-for-capacity-planning-guidance"></a>Базовые требования для руководства по планированию ресурсов

В этой статье предполагается наличие следующих базовых требований:

- Читатели прочитали и знакомы с [рекомендациями по настройке производительности для Windows Server 2012 R2](https://docs.microsoft.com/previous-versions//dn529133(v=vs.85)).
- Платформа Windows Server является архитектурой на основе x64. Но даже если ваша среда Active Directory установлена на Windows Server 2003 x86 (теперь за пределами жизненного цикла поддержки) и имеет дерево сведений о каталоге (DIT) размером менее 1,5 ГБ и может легко храниться в памяти, рекомендации из этого статья по-прежнему применима.
- Планирование ресурсов — непрерывный процесс, и следует регулярно проверять, насколько хорошо среда является ожидаемой.
- Оптимизация будет выполняться для нескольких жизненных циклов оборудования по мере изменения затрат на оборудование. Например, память снижается, стоимость ядра уменьшается или изменяется цена различных вариантов хранения.
- Спланируйте пиковый период занятости дня. Рекомендуется рассмотреть это значение в течение 30 минут или часов. Что-то больше может скрыть реальные пиковые значения, и что-то меньше, может быть искажено "временными пиковыми нагрузками".
- Планируйте рост на протяжении всего жизненного цикла оборудования для предприятия. Это может быть стратегия обновления или добавления оборудования в шахматном порядке или полное обновление каждые три-пять лет. Каждый из них потребует "предположения", как объем нагрузки на Active Directory будет расти. Исторические данные, если они собраны, помогут в этой оценке. 
- Спланируйте отказоустойчивость. После того как оценка *N* будет получена, спланируйте сценарии, включающие *n* &ndash; 1 *, n* &ndash; 2, *n* &ndash; *x*.
  - Добавьте дополнительные серверы в соответствии с нуждами организации, чтобы гарантировать, что потери одного или нескольких серверов не превышают максимальные значения максимальной емкости.
  - Также учтите, что план роста и план отказоустойчивости должны быть интегрированы. Например, если для поддержки нагрузки требуется один контроллер домена, но оценка заключается в том, что нагрузка будет удвоена в следующем году и потребовала всего два DCs, для поддержки отказоустойчивости не будет достаточно емкости. Решение будет начинаться с трех контроллеров доменов. Можно также запланировать Добавление третьего контроллера домена после 3 или 6 месяцев, если бюджеты нестрогие.

    >[!NOTE]
    >Добавление приложений, поддерживающих Active Directory, может оказать заметное влияние на нагрузку на контроллеры домена, а также на то, поступает ли нагрузка с серверов приложений или клиентов.

### <a name="three-step-process-for-the-capacity-planning-cycle"></a>Двухэтапный процесс для цикла планирования емкости

При планировании емкости сначала решите, какое качество обслуживания необходимо. Например, основной центр обработки данных поддерживает более высокий уровень параллелизма и требует более согласованной работы пользователей и приложений, что требует более тщательного внимания к избыточности и минимизации узких мест системы и инфраструктуры. Напротив, вспомогательное расположение с несколькими пользователями не требует одинакового уровня параллелизма или отказоустойчивости. Таким образом, вспомогательному офису может не требоваться много внимания на оптимизации базового оборудования и инфраструктуры, что может привести к экономии затрат. Все рекомендации и инструкции предназначены для обеспечения оптимальной производительности и могут быть выборочно ослаблены для сценариев с менее требовательными требованиями.

Следующий вопрос: виртуализированный или физический? С точки зрения планирования емкости нет правильного или неверного ответа; Существует только другой набор переменных для работы. В сценариях виртуализации можно перейти к одному из двух вариантов:

- "Прямое сопоставление" с одним гостем на узел (где виртуализация существует только для абстракции физического оборудования с сервера)
- "Общий узел"

Сценарии тестирования и рабочей среды указывают на то, что сценарий "прямого сопоставления" может обрабатываться идентично физическому узлу. «Общий узел», тем не менее, порождает ряд рекомендаций, которые более подробно описаны далее. Сценарий "общий узел" означает, что AD DS также конкурирует за ресурсы, и для этого существуют штрафы и соображения по настройке.

Учитывая эти соображения, цикл планирования ресурсов — это итеративный процесс, сопоставленный в три этапа:

1. Измерьте существующую среду, определите, где в настоящее время находятся узкие места системы, и получите основные сведения о среде, необходимые для планирования необходимого объема ресурсов.
1. Определите необходимое оборудование в соответствии с критерием, описанным в шаге 1.
1. Отслеживайте и проверяйте, что инфраструктура реализована в соответствии с спецификациями. Некоторые данные, собранные на этом шаге, становятся базовыми показателями для следующего цикла планирования ресурсов.

### <a name="applying-the-process"></a>Применение процесса

Чтобы оптимизировать производительность, убедитесь, что эти основные компоненты правильно выбраны и настроены для загрузки приложения:

1. Память
1. Network
1. Служба хранилища
1. Процессор
1. Вход в сеть

Основные требования к хранению AD DS и общее поведение хорошо написанного клиентского программного обеспечения позволяют средам с числом пользователей от 10 000 до 20 000, наблюдающих значительные инвестиции в планирование емкости, в соответствии с физическим оборудованием, как практически любой современный сервер. система класса будет управлять нагрузкой. С другой стороны, в следующей таблице приведены сводные сведения о том, как оценить существующую среду, чтобы выбрать нужное оборудование. Каждый компонент подробно анализируется в последующих разделах, чтобы помочь AD DS администраторам оценивать свою инфраструктуру с помощью базовых рекомендаций и субъектов, относящихся к конкретной среде.

Как правило:

- Любое изменение размера, основанное на текущих данных, будет точным только для текущей среды.
- Для каких бы то ни было оценок, следует рассчитывать на рост спроса на жизненный цикл оборудования.
- Определите, следует ли слишком сегодня и дорастись в более крупной среде, или добавьте емкость в течение жизненного цикла.
- Для виртуализации применяются все те же субъекты и методологии планирования емкости, за исключением того, что издержки на виртуализацию необходимо добавлять к любому домену, связанному с доменом.
- Планирование ресурсов, как и все, что попытается предсказать, не является точным науком. Не стоит рассчитывать на то, что следует вычислять идеально и с точностью 100%. Здесь приведены рекомендации по экономичности. Добавьте емкость для дополнительной безопасности и непрерывно проверяйте, что среда остается на целевом объекте.

### <a name="data-collection-summary-tables"></a>Сводные таблицы сбора данных

#### <a name="new-environment"></a>Новая среда

| Component | Оценки |
|-|-|
|Размер хранилища или базы данных|40 КБ – 60 КБ для каждого пользователя|
|ОЗУ|Размер базы данных<br />Рекомендации по основным операционным системам<br />Сторонние приложения|
|Network|1 ГБ|
|ЦП|1000 параллельных пользователей для каждого ядра|

#### <a name="high-level-evaluation-criteria"></a>Условия оценки высокого уровня

| Component | Критерии оценки | Факторы, которые следует учесть при планировании |
|-|-|-|
|Размер хранилища или базы данных|Раздел «Включение ведения журнала дискового пространства, освобожденного путем дефрагментации» в разделе [ограничения хранилища](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))| |
|Производительность хранилища и базы данных|<ul><li>"LogicalDisk ( *\<диск\>базы данных NTDS*) \авг, чтение, запись с диска (с)" логический диск ( *\<диск\>базы данных NTDS*) \авг диск сек/Write "" LogicalDisk ( *\<диск базы данных NTDS )\>* \Авг диск с/переносом "</li><li>"LogicalDisk ( *\<диск\>базы данных NTDS*) \ операций чтения/с", "LogicalDisk ( *\<\>* *\<диск базы данных NTDS) \ операций записи в секунду", "логический диск\>базыданныхNTDS*) \ Передач/с "</li></ul>|<ul><li>Существует две проблемы, с которыми следует обращаться в хранилище.<ul><li>Доступное пространство, которое с размером современного дискового накопителя и хранилища на основе SSD, не имеет значения для большинства сред AD.</li> <li>Доступные операции ввода-вывода (IO) — во многих средах это часто бывает невнимательно. Но важно оценить только те среды, в которых недостаточно оперативной памяти для загрузки всей базы данных NTDS в память.</li></ul><li>Хранилище может быть сложным разделом и должно содержать опыт поставщиков оборудования для правильного изменения размера. Особенно с более сложными сценариями, такими как SAN, NAS и сценарии iSCSI. Однако в общем случае стоимость за каждый гигабайт хранилища часто оппозициейся на каждую операцию ввода-вывода:<ul><li>RAID 5 имеет более низкие затраты на гигабайт, чем RAID 1, но RAID 1 имеет более низкие затраты на операции ввода-вывода.</li><li>Жесткие диски на основе дисковых накопителей имеют меньшую стоимость в гигабайтах, но в SSDs снижена стоимость операций ввода-вывода.</li></ul><li>После перезапуска компьютера или службы домен Active Directory Services кэш расширяемого хранилища (ESE) остается пустым, и производительность будет привязана к диску, пока кэш загревается.</li><li>В большинстве сред Active Directory считывает интенсивность операций ввода-вывода в случайном шаблоне на диски, что значительно снижает эффективность кэширования и оптимизации операций чтения.  Кроме того, AD имеет больший объем кэша в памяти, чем большинство кэшей системы хранения.</li></ul>
|ОЗУ|<ul><li>Размер базы данных</li><li>Рекомендации по основным операционным системам</li><li>Сторонние приложения</li></ul>|<ul><li>Хранилище — это самый медленный компонент компьютера. Чем больше возможностей может налагаться в оперативной памяти, тем меньше места необходимо для перехода на диск.</li><li>Обеспечьте выделение достаточного объема ОЗУ для хранения операционной системы, агентов (Антивируса, резервного копирования, мониторинга), базы данных NTDS и роста с течением времени.</li><li>Для сред, в которых максимальное значение объема ОЗУ не экономично (например, для вспомогательных расположений) или нецелесообразно (расположение DIT слишком велико), сослаться на раздел Storage, чтобы обеспечить правильное изменение размера хранилища.</li></ul>|
|Network|<ul><li>"Сетевой интерфейс (\*) получено \/с"</li><li>"Сетевой интерфейс (\*) отправлено \/с"|<ul><li>Как правило, трафик, отправляемый с контроллера домена, намного превышает трафик, передаваемый контроллеру домена.</li><li>Так как коммутируемое Ethernet-подключение является дуплексным, входящий и исходящий сетевой трафик необходимо изменять независимо от размера.</li><li>Консолидация количества контроллеров домена увеличит объем пропускной способности, используемой для отправки ответов обратно в клиентские запросы для каждого контроллера домена, но будет достаточно близко к линейному для всего сайта.</li><li>Если вы удалите контроллеры домена спутникового расположения, не забудьте добавить пропускную способность для вспомогательного контроллера домена в контроллеры концентратора, а также использовать его для определения объема трафика глобальной сети.</li></ul>|
|ЦП|<ul><li>"Логический диск ( *\<диск\>базы данных NTDS*) \авг диск с/чтение"</li><li>"Процесс (LSASS)\\% загруженности процессора"</li></ul>|<ul><li>После исключения хранилища в качестве узкого места устраните требуемый объем мощности вычислений.</li><li>Хотя это не очень линейное, количество процессорных ядер, потребляемое на всех серверах в определенной области (например, на сайте), можно использовать для оценки количества процессоров, необходимых для поддержки общей нагрузки клиента. Добавьте минимальное необходимое для поддержания текущего уровня обслуживания во всех системах в пределах области.</li><li>Изменения в тактовой частоте процессора, включая изменения, связанные с управлением питанием, Номера влияния, полученные из текущей среды. Как правило, невозможно точно оценить, как переход с процессора с частотой 2,5 ГГц на процессор с частотой до 3 ГГц снизит число требуемых процессоров.</li></ul>|
|NetLogon|<ul><li>"Служба Netlogon\*() \семафоре получает"</li><li>"Netlogon (\*) \семафоре Timeouts"</li><li>"Netlogon (\*) \авераже время удержания семафора"</li></ul>|<ul><li>Сетевой вход в систему Secure Channel/MaxConcurrentAPI влияет только на среды с проверкой подлинности NTLM и/или проверкой PAC. Проверка PAC включена по умолчанию в версиях операционной системы до Windows Server 2008. Это параметр клиента, поэтому контроллеры домена будут затронуты, пока он не будет отключен на всех клиентских системах.</li><li>Среды с значительной проверкой подлинности между лесами, в том числе отношения доверия внутри леса, имеют больший риск при неправильном размере.</li><li>При консолидации серверов будет увеличен параллелизм проверки подлинности с перекрестным доверием.</li><li>Необходимо учитывать всплески, например отработку отказа кластера, так как пользователи повторно проходят проверку подлинности в новом узле кластера.</li><li>Для отдельных клиентских систем (например, кластеров) может потребоваться настройка.</li></ul>|

## <a name="planning"></a>Планирование

В течение длительного времени рекомендации сообщества по изменению размера AD DS были «помещают объем ОЗУ как размер базы данных». В большинстве случаев эта рекомендация полностью важна для большинства сред. Но AD DS экосистема значительно больше, так как сами среды AD DS, так как они познакомится с 1999. Несмотря на то что увеличение мощности вычислений и коммутатора с архитектур x86 до архитектуры x64 сделали более тонкие аспекты определения производительности, несущественные для большего количества клиентов, работающих AD DS на физическом оборудовании, рост виртуализации повторно предоставили проблемы настройки для более крупной аудитории, чем раньше.

Ниже приведены рекомендации по определению и планированию требований Active Directory как услуги независимо от того, развернута ли она в физическом, виртуальном или физическом наборе или в чисто виртуализированном сценарии. Таким образом, мы будем разбивать оценку на каждый из четырех основных компонентов: хранилища, памяти, сети и процессора. Вкратце, чтобы максимально повысить производительность AD DS, цель заключается в максимальном приближении к привязанному процессору.

## <a name="ram"></a>ОЗУ

В то же самое, чем больше данных можно кэшировать в ОЗУ, тем меньше места необходимо для перехода на диск. Чтобы максимально увеличить масштабируемость сервера, минимальный объем ОЗУ должен быть равен сумме текущего размера базы данных, общего размера SYSVOL, рекомендуемого объема операционной системы и рекомендаций поставщика для агентов (Антивирус, мониторинг, резервное копирование и т. д.). ). Чтобы обеспечить рост времени существования сервера, необходимо добавить дополнительную сумму. Это будет предметно субъективным в зависимости от оценок роста базы данных на основе изменений в окружающей среде.

Для сред, в которых максимальное значение объема ОЗУ не является экономичным (например, вспомогательными расположениями) или неразумным (DIT слишком большой), сослаться на раздел "хранилище", чтобы убедиться, что хранилище правильно спроектировано.

В общем контексте, который входит в общую область памяти, определяется размер файла подкачки. В том же контексте, что и остальная часть памяти, цель заключается в том, чтобы снизить число переходов на более медленный диск. Поэтому вопрос должен быть в разделе «как следует изменять размер файла подкачки?». в «какой объем ОЗУ требуется для минимального разбиения на страницы?». Ответ на последний вопрос описан в оставшейся части этого раздела. Это оставляет большую часть обсуждения по изменению размера файла подкачки до области общих рекомендаций операционной системы и необходимости настройки системы для дампов памяти, которые не связаны с производительностью AD DS.

### <a name="evaluating"></a>Вычисляет

Объем ОЗУ, который требуется контроллеру домена (DC), на самом деле является сложным упражнением по следующим причинам:

- Высокая вероятность ошибок при попытке использовать существующую систему для оценки необходимого объема ОЗУ, так как LSASS будет обрезаться с учетом условий нехватки памяти, искусственно изменяя потребность.
- Субъективный факт, что отдельный контроллер домена должен кэшировать только то, что интересно для его клиентов. Это означает, что данные, которые необходимо кэшировать на контроллере домена с сервером Exchange, будут сильно отличаться от данных, которые необходимо кэшировать на контроллере домена, который выполняет проверку подлинности только для пользователей.
- Трудозатраты по оценке ОЗУ для каждого контроллера домена с учетом регистра являются недопустимыми и изменяются при изменении среды.
- Критерии, стоящие за рекомендацией, помогут принимать взвешенные решения. 
- Чем больше возможностей кэшируется в ОЗУ, тем меньше места необходимо для перехода на диск. 
- Хранилище — это самый медленный компонент компьютера. Доступ к данным на дисках с дисковым накопителем и накопителями на ТВЕРДОТЕЛЬном носителе находится в порядке 1, 000, 1000 раз медленнее, чем доступ к данным в ОЗУ.

Таким же, чтобы максимально повысить масштабируемость сервера, минимальным объемом ОЗУ является сумма текущего размера базы данных, общий размер SYSVOL, рекомендуемый объем операционной системы и рекомендации поставщика для агентов (Антивирус, мониторинг, резервное копирование, и т. д.). Добавьте дополнительные суммы, чтобы обеспечить рост времени существования сервера. Это будет предметно субъективное окружение на основе оценок роста базы данных. Однако для вспомогательных расположений с небольшим набором конечных пользователей эти требования могут быть ослаблены, так как эти сайты не нуждаются в кэшировании, чтобы обслуживать большинство запросов.

Для сред, в которых максимальное значение объема ОЗУ не экономично (например, для вспомогательных расположений) или нецелесообразно (расположение DIT слишком велико), сослаться на раздел Storage, чтобы обеспечить правильное изменение размера хранилища.

> [!NOTE]
> При изменении размера файла подкачки размер памяти будет отбросить. Поскольку цель состоит в том, чтобы снизить скорость перехода на более медленный диск, вопрос «как следует изменять файл подкачки?». в «какой объем ОЗУ требуется для минимального разбиения на страницы?». Ответ на последний вопрос описан в оставшейся части этого раздела. Это оставляет большую часть обсуждения по изменению размера файла подкачки до области общих рекомендаций операционной системы и необходимости настройки системы для дампов памяти, которые не связаны с производительностью AD DS.

### <a name="virtualization-considerations-for-ram"></a>Рекомендации по виртуализации для ОЗУ

Избегайте чрезмерной фиксации памяти на узле. Основная цель оптимизации объема ОЗУ заключается в том, чтобы максимально сокращать количество времени, затрачиваемого на работу с диском. В сценариях виртуализации существует концепция чрезмерного объема памяти, при которой на физическом компьютере выделяется больше оперативной памяти для гостей. Сам по себе это не проблема. Это может быть проблемой, когда общий объем памяти, активно используемый всеми гостями, превышает объем ОЗУ на узле, а базовый узел начинает подкачка. Производительность становится привязанной к диску в случаях, когда контроллер домена переходит в NTDS. dit для получения данных, или если контроллер домена переходит в файл подкачки для получения данных, или узел переходит на диск, чтобы получить данные, которые подсчитает гостевой компьютер в оперативной памяти.

### <a name="calculation-summary-example"></a>Пример сводки вычислений

|Component|Предполагаемая память (пример)|
|-|-|
|Рекомендуемый объем ОЗУ базовой операционной системой (Windows Server 2008)|2 ГБ|
|Внутренние задачи LSASS|200 МБ|
|Агент мониторинга|100 МБ|
|Антивирусная программа|100 МБ|
|База данных (глобальный каталог)|8,5 ГБ вы уверены???|
|Подкладка для запуска резервного копирования, администраторы могут войти в систему без влияния|1 ГБ|
|Total (Всего)|12 ГБ|

**Такую 16 ГБ**

С течением времени предположение можно сделать, чтобы в базу данных добавлялись дополнительные данные, а сервер, вероятно, находится в рабочей среде в течение 3 – 5 лет. На основе оценки роста 33% 16 ГБ будет разумным объемом ОЗУ для размещения на физическом сервере. В виртуальной машине, учитывая, какие параметры можно изменить, а оперативную память можно добавить на виртуальную машину, начиная с 12 ГБ, вы можете использовать план для отслеживания и обновления в будущем.

## <a name="network"></a>Network

### <a name="evaluating"></a>Вычисляет
Этот раздел является меньшей, чем Оценка требований к трафику репликации, который фокусируется на трафике, проходящего через глобальную сеть, и подробно рассматривается в [Active Directory трафика репликации](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10)), чем в случае оценки общей пропускной способности и сети. Требуемая емкость, включая клиентские запросы, групповая политика приложения и т. д. Для существующих сред это можно сделать с помощью счетчиков производительности "сетевой интерфейс (\*) \ получено/с" и "сетевой интерфейс (\*) \ отправлено/с". Интервалы выборки для счетчиков сетевых интерфейсов — 15, 30 или 60 минут. Что-то меньше, как правило, слишком изменчиво для приемлемых измерений; чем больше, тем более плавное количество просматривается слишком часто.

> [!NOTE]
> Как правило, большая часть сетевого трафика на контроллере домена является исходящим, так как контроллер домена отвечает на клиентские запросы. Это причина, по которой основное внимание уделяется исходящему трафику, хотя рекомендуется оценивать каждую среду для входящего трафика. Те же подходы можно использовать для решения и проверки требований к входящему сетевому трафику. Дополнительные сведения см. в статье [базы знаний 929851: Диапазон динамических портов по умолчанию для TCP/IP изменился в Windows Vista и Windows Server 2008](http://support.microsoft.com/kb/929851).

### <a name="bandwidth-needs"></a>Требования к пропускной способности

Планирование масштабируемости сети охватывает две различные категории: объем трафика и загрузку ЦП из сетевого трафика. Каждый из этих сценариев прямо вперед по сравнению с некоторыми другими подразделами в этой статье.

В оценке того, какой объем трафика должен поддерживаться, существует две уникальные категории планирования емкости для AD DS с точки зрения сетевого трафика. Первый — трафик репликации, который проходит между контроллерами домена и подробно рассматривается в справочнике по [трафику Active Directory репликации](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10)) и по-прежнему относится к текущим версиям AD DS. Второй — это трафик внутрисайтовой клиент-сервер. Один из более простых сценариев, в котором планируется планирование, промежуточный трафик получает небольшие запросы от клиентов, относящихся к большим объемам данных, отправляемых обратно клиентам. 100 МБ обычно достаточно в средах до 5 000 пользователей на сервер в сайте. Для любого свыше 5 000 пользователей рекомендуется использовать сетевой адаптер размером 1 ГБ и поддержку масштабирования на стороне приема (RSS). Чтобы проверить этот сценарий, особенно при использовании сценариев консолидации серверов, просмотрите сетевой интерфейс (\*) \ байт/сек для всех контроллеров домена на сайте, добавьте их вместе и разделите их на целевое количество контроллеров доменов, чтобы убедиться в наличии является достаточной емкостью. Самый простой способ сделать это — использовать представление с областями с накоплением в мониторе надежности и производительности Windows (прежнее название — PerfMon), убедившись, что все счетчики масштабируются одинаково.

Рассмотрим следующий пример (также известный как, действительно сложный способ проверки применимости общего правила к конкретной среде). Выполняются следующие предположения.

- Цель состоит в том, чтобы уменьшить объем памяти до минимально возможного количества серверов. В идеале один сервер будет выполнять нагрузку, а дополнительный сервер развертывается для обеспечения избыточности (сценарий *N* + 1). 
- В этом сценарии текущий сетевой адаптер поддерживает только 100 МБ и находится в переключенной среде.  
  Максимальная Целевая Загрузка пропускной способности сети составляет 60% в сценарии N (потери контроллера домена).
- К каждому серверу подключено около 10 000 клиентов.

База знаний, полученная на основе данных в диаграмме (\*сетевой интерфейс () \ отправлено/с):

1. Рабочий день начинается с 5:30 и подойдет к концу вниз в 7:00 РМ.
1. Пиковая максимальная загруженность — от 8:00 до 8:15 AM, в которой более 25 байт отправлено в секунду на пиковый контроллер домена.  
   > [!NOTE]
   > Все данные производительности являются историческими. Так что Пиковая точка данных в 8:15 указывает на загрузку с 8:00 до 8:15.
1. Наблюдаются пики, предшествующие 4:00, с загрузкой более 20 байтов в секунду на наиболее загруженном контроллере домена, что может указывать на загрузку из разных часовых поясов или на фоновую инфраструктуру, например резервное копирование. Так как пиковое значение 8:00 AM превышает это действие, оно не имеет значения.
1. На сайте имеется пять контроллеров домена.
1. Максимальная нагрузка составляет около 5,5 МБ/с на контроллере домена, который представляет 44% подключения 100 МБ. Используя эти данные, можно оценить, что общая пропускная способность, необходимая между 8:00 и 8:15 AM, составляет 28 МБ/с.
   >[!NOTE]
   >Будьте внимательны с тем, что счетчики отправленных и полученных сетевых интерфейсов находятся в байтах, а пропускная способность сети измеряется в битах. 100 МБ &divide; 8 = 12,5 МБ, 1 ГБ &divide; 8 = 128 МБ.
  
Выводы

1. Эта текущая среда соответствует требованиям к отказоустойчивости на уровне N + 1 при использовании целевых объектов в 60%. Отключение одной системы переместит пропускную способность на сервер с примерно 5,5 МБ/с (44%) около 7 МБ/с (56%).
1. На основе ранее указанной цели консолидации на одном сервере это значение превышает максимальное использование целевого объекта и теоретически может быть занято подключением 100 МБ.
1. С подключением 1 ГБ это будет представлять 22% общей емкости.
1. При нормальных условиях работы в сценарии *N* + 1 нагрузка клиента будет относительно равномерно распространяться примерно на 14 МБ/с на сервер или на 11% от общей емкости.
1. Чтобы обеспечить достаточную емкость во время недоступности контроллера домена, нормальная работа на сервере будет равна 30% использованию сети или 38 Мб/с на сервер. Цели отработки отказа будут 60% использования сети или 72 МБ/с на сервер.  
  
Вкратце, окончательное развертывание систем должно иметь сетевой адаптер 1 ГБ и быть подключен к сетевой инфраструктуре, которая будет поддерживать такую нагрузку. Кроме того, обратите внимание, что учитывая объем создаваемого сетевого трафика, загрузка ЦП от сетевых подключений может оказать значительное влияние и ограничить максимальную масштабируемость AD DS. Этот же процесс можно использовать для оценки объема входящего подключения к контроллеру домена. Но учитывая предоминанце исходящего трафика относительно входящего трафика, это учебное упражнение для большинства сред. Обеспечение аппаратной поддержки для RSS важно в средах с более чем 5 000 пользователями на сервер. Для сценариев с высоким сетевым трафиком балансировка нагрузки прерываний может быть узким местом. Это может быть обнаружено процессором\*(\% ), равномерно распределенным по процессорам. Сетевые адаптеры с поддержкой RSS могут уменьшить это ограничение и повысить масштабируемость.

> [!NOTE]
> Аналогичный подход можно использовать для оценки дополнительной емкости, необходимой для консолидации центров обработки данных, или снятия контроллера домена в вспомогательном расположении. Просто собирайте исходящий и входящий трафик для клиентов, который будет представлять собой объем трафика, который теперь будет присутствовать в каналах WAN.  
>  
> В некоторых случаях может возникнуть больше трафика, чем ожидалось, так как трафик выполняется медленнее, например, если проверка сертификата не соответствует агрессивному времени ожидания в глобальной сети. По этой причине размер и использование глобальной сети должны быть итеративным, непрерывным процессом.

### <a name="virtualization-considerations-for-network-bandwidth"></a>Рекомендации по виртуализации для пропускной способности сети

Делать рекомендации для физического сервера несложно: 1 ГБ для серверов, поддерживающих более 5000 пользователей. Когда несколько гостей начинают совместно использовать базовую инфраструктуру виртуального коммутатора, необходимо дополнительное внимание, чтобы обеспечить достаточную пропускную способность узла для поддержки всех гостей в системе и, таким образом, требует дополнительного бесполезного. Это не так много, как расширение, обеспечивающее сетевую инфраструктуру на размещающем компьютере. Это независимо от того, является ли сеть включающей контроллер домена, работающий в качестве гостевой виртуальной машины на узле с сетевым трафиком, который проходит через виртуальный коммутатор, или напрямую подключен к физическому коммутатору. Виртуальный коммутатор — это еще один компонент, в котором канал исходящей связи должен поддерживать объем передаваемых данных. Поэтому физический сетевой адаптер физического узла, связанный с коммутатором, должен поддерживать загрузку контроллера домена, а также все остальные Гостевые виртуальные коммутаторы, подключенные к физическому сетевому адаптеру.

### <a name="calculation-summary-example"></a>Пример сводки вычислений

|Система|Пиковая пропускная способность|
|-|-|
КОНТРОЛЛЕР ДОМЕНА 1|6,5 МБ/с|
КОНТРОЛЛЕР ДОМЕНА 2|6,25 МБ/с|
|КОНТРОЛЛЕР ДОМЕНА 3|6,25 МБ/с|
|КОНТРОЛЛЕР ДОМЕНА 4|5,75 МБ/с|
|КОНТРОЛЛЕР ДОМЕНА 5|4,75 МБ/с|
|Total (Всего)|28,5 МБ/с|

**Такую 72 МБ/с** (28,5 МБ/с, деленная на 40%)

|Число целевых систем|Общая пропускная способность (из выше)|
|-|-|
|2|28,5 МБ/с|
|Результирующее нормальное поведение|28,5 &divide; 2 = 14,25 МБ/с|

Как всегда, с течением времени можно предположить, что нагрузка клиента увеличится, и этот рост следует планировать как можно лучше. Рекомендуемый объем для планирования позволяет оценить рост сетевого трафика 50%.

## <a name="storage"></a>Служба хранилища

Хранилище планирования состоит из двух компонентов:

- Емкость или размер хранилища
- Производительность

Очень много времени и документации посвящены планированию мощности, что значительно отменяет производительность. В связи с текущими затратами на оборудование большинство сред не настолько велики, что один из них на самом деле является проблемой, а рекомендуемый вариант «полагаться на объем ОЗУ как размер базы данных» обычно охватывает все остальное, хотя это может оказаться избыточным для вспомогательных расположений в большей степени. возможным.

### <a name="sizing"></a>Изменения размера

#### <a name="evaluating-for-storage"></a>Оценка хранилища

По сравнению с 13 лет назад, когда было введено Active Directory, время, когда 4 ГБ и 9 ГБ были наиболее распространенными размерами дисков, размер Active Directory не стоит даже учитывать для всех самых крупных сред. Благодаря минимальным объемам доступных жестких дисков в диапазоне 180 ГБ вся операционная система, SYSVOL и NTDS. dit можно легко разместить на одном диске. Поэтому рекомендуется не использовать в этой области трудовые инвестиции.

Единственная рекомендация заключается в том, чтобы обеспечить наличие 110% размера NTDS. dit, чтобы включить дефрагментацию. Кроме того, должно быть создано место для увеличения времени существования оборудования.

Первое и самое важное замечание — Оценка размера NTDS. dit и SYSVOL. Эти измерения приводят к определению размера выделенного дискового пространства и объема ОЗУ. Из-за нехватки (относительной) низкой стоимости этих компонентов математические вычисления не требуются. Сведения о том, как оценить это как для существующих, так и для новых сред, можно найти в статье [хранение данных](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961771(v=technet.10)) в ряде статей. В частности, см. следующие статьи:

- В **существующих средах &ndash;**  в разделе "Включение ведения журнала дискового пространства, освобожденного путем дефрагментации" раздела " [ограничения хранилища](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))статей".
- **Для новых сред &ndash;**  статья содержит [оценки роста для Active Directory пользователей и](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc961779(v=technet.10))подразделений.

  > [!NOTE]
  > Статьи основаны на оценках размера данных, сделанных во время выпуска Active Directory в Windows 2000. Используйте размеры объектов, отражающие фактический размер объектов в среде.

При просмотре существующих сред с несколькими доменами могут быть различия в размерах баз данных. Если это так, используйте наименьший размер глобального каталога (GC) и не-GC.  

Размер базы данных может различаться в зависимости от версии операционной системы. Контроллеры доменов, работающие под управлением более ранних операционных систем, например Windows Server 2003, имеют меньший размер базы данных, чем контроллер домена, работающий под управлением более поздней операционной системы, такой как Windows Server 2008 R2, особенно если включены такие функции, как Active Directory корзина или перемещение учетных данных.

> [!NOTE]  
  >
>- Для новых сред Обратите внимание на то, что оценки роста для Active Directory пользователей и подразделений указывают на то, что 100 000 пользователей (в том же домене) потребляют около 450 МБ пространства. Обратите внимание, что заполненные атрибуты могут оказать огромное влияние на общую сумму. Атрибуты будут заполняться многими объектами как продуктами сторонних разработчиков, так и Microsoft, включая Microsoft Exchange Server и Lync. Для оценки, основанной на портфеле продуктов в среде, предпочтительно, но в этом упражнении не стоит тратить много времени и усилий на тестирование математических вычислений и тестирования для получения точных оценок для всех сред, кроме самых крупных.
>- Убедитесь, что 110% размера NTDS. dit доступно в качестве свободного пространства, чтобы включить автономную дефрагментацию, и запланируйте рост на период с 3 до пяти лет. Учитывая, насколько дорогое хранилище, оценка хранилища на 300% размера каталога DIT в размере для хранения данных может быть надежным для обеспечения роста и возможной необходимости в автономной дефрагментации.

#### <a name="virtualization-considerations-for-storage"></a>Рекомендации по виртуализации для хранилища

В сценарии, где несколько файлов виртуального жесткого диска (VHD) выделены на одном томе, используйте диск фиксированного размера размером не менее 210% (100% свободного места на диске DIT + 110%), чтобы убедиться в наличии достаточного места в зарезервированном пространстве.  

#### <a name="calculation-summary-example"></a>Пример сводки вычислений

|Данные, собранные на этапе оценки| |
|-|-|
|Размер NTDS. dit|35 ГБ|
|Модификатор для разрешения автономной дефрагментации|2.1|
|Общий объем необходимого хранилища|73,5 ГБ|

> [!NOTE]
> Это необходимое хранилище является дополнением к хранилищу, необходимому для SYSVOL, операционной системы, файла подкачки, временных файлов, локальных кэшированных данных (например, файлов установщика) и приложений.

### <a name="storage-performance"></a>Производительность хранилища

#### <a name="evaluating-performance-of-storage"></a>Оценка производительности хранилища

Как самый медленный компонент в пределах любого компьютера, хранилище может иметь самое большое негативное воздействие на работу клиента. Для тех сред, которые достаточно велики для того, чтобы получить рекомендации по размерам оперативной памяти, могут быть девастатинг.  Кроме того, сложность и разновидность технологии хранения еще более увеличивают риск сбоя, так как важность «размещения операционной системы, журналов и базы данных» на отдельных физических дисках ограничена в полезных сценариях.  Это связано с тем, что долгосрочная рекомендация основывается на предположении, что «диск» является выделенной дискетой, и это позволило бы изолировать операции ввода-вывода.  Это предположение, которое делает это значение истинным, больше не связано с введением:

- Новые типы хранилищ, виртуальные и общие сценарии хранилища
- Общие диски в сети хранения данных (SAN)
- VHD-файл в сети SAN или хранилище, подключенном к сети
- Твердотельные накопители
- Многоуровневые архитектуры хранилища (т. е. кэширование на уровне хранилища SSD, превышающие хранилище на основе дискового накопителя)

Вкратце, конечная цель всех усилий по производительности хранилища, независимо от базовой архитектуры и структуры хранилища, заключается в том, чтобы обеспечить доступность требуемого объема операций ввода-вывода в секунду и их выполнение в течение допустимого периода времени. . В этом разделе описано, как оценить требования к AD DS базовому хранилищу, чтобы обеспечить правильную разрабатывать решения для хранения.  Учитывая изменчивость современных технологий хранения данных, лучше работать с поставщиками хранилища, чтобы обеспечить достаточное количество операций ввода-вывода.  В этих сценариях с локально подключенным хранилищем обратитесь к приложению C, чтобы ознакомиться с основами разработки традиционных сценариев локального хранилища.  Эти участники обычно применимы к более сложным уровням хранилища. Кроме того, они могут помочь в диалоге с поставщиками, поддерживающими серверные решения для хранения данных.

- Учитывая широкий спектр доступных вариантов хранения, рекомендуется присваивать опыту группы поддержки оборудования или поставщиков, чтобы убедиться, что конкретное решение соответствует потребностям AD DS. Следующие числа представляют собой сведения, которые будут предоставлены специалистам по хранению.

Для сред, в которых база данных слишком велика для хранения в ОЗУ, используйте счетчики производительности, чтобы определить, какой объем операций ввода-вывода должен поддерживаться:

- LogicalDisk (\*) \авг чтения с диска (с) (например, если NTDS. dit хранится на диске D:/ диск, полный путь будет иметь логический диск (D:) \Авг/чтение с диска в секунду).
- LogicalDisk (\*) \авг запись на диск (сек.)
- Логический диск\*() \авг/обмен
- Логический диск\*() \ операций чтения в секунду
- Логический диск\*() \ операций записи в секунду
- Логический диск\*() \ передач/с

Их следует вычислить с интервалом в 15/30/60 минут, чтобы вычислить показатели требований текущей среды.

#### <a name="evaluating-the-results"></a>Оценка результатов

> [!NOTE]
> Основное внимание уделяется чтению базы данных, так как это, как правило, самый ресурсоемкий компонент, та же логика может применяться для записи в файл журнала путем подстановки LogicalDisk ( *\<журнал\>NTDS*) \авг дискового пространства (в секунду), записи и LogicalDisk (*Журнал\>NTDS) \ операций записи в секунду:\<*
>  
> - Логический диск *\<(\>NTDS*) \авг. чтение с диска (с) указывает, достаточно ли размера текущего хранилища.  Если результаты приблизительно равны времени доступа к диску для типа диска, логический диск ( *\<NTDS\>* ) \ операций чтения/с является допустимой мерой.  Проверьте спецификации производителя для хранилища в серверной части, но допустимые диапазоны для дискового пространства ( *\<NTDS\>* ) \авг:
>   - от 7200 – 9 до 12,5 миллисекунд (МС)
>   - от 10 000 – 6 до 10 мс
>   - 15 000 – 4 – 6 мс  
>   - SSD – от 1 до 3 мс  
>   - >[!NOTE]
>     >Существуют рекомендации по снижению производительности хранилища в 15ms до 20 мс (в зависимости от источника).  Разница между приведенными выше значениями и другими рекомендациями заключается в том, что указанные выше значения являются обычным диапазоном эксплуатации.  Другие рекомендации — это руководство по устранению неполадок, которое позволяет узнать, когда работа клиента значительно снизится и станет заметным.  Подробное описание см. в приложении на языке C.
> - Логический диск *\<(\>NTDS*) \ операций чтения/с — это объем выполняемых операций ввода-вывода.
>   - Если логический диск *\<(\>NTDS*) \авг для чтения с диска (сек.) находится в оптимальном диапазоне для внутреннего хранилища, то для изменения размера хранилища можно напрямую использовать LogicalDisk ( *\<NTDS\>* ) \ операций чтения/с.
>   - Если логический диск *\<(\>NTDS*) \авг. чтение с диска (с) не находится в оптимальном диапазоне для серверного хранилища, требуется дополнительный ввод-вывод в соответствии со следующей формулой:
>     > (Логический диск *\<(\>NTDS*) \авг время &times; чтения с диска &divide; (с)) (доступ к физическому носителю) (логический диск) (LogicalDisk ( *\<NTDS\>* ) \авг время чтения с диска (с))

Замечания:

- Обратите внимание, что если сервер настроен с использованием неоптимального объема ОЗУ, эти значения будут неточными для целей планирования.  Они будут ошибочно отображаться на высоком уровне и по-прежнему могут использоваться в качестве наихудших ситуаций.
- Добавление или оптимизация оперативной памяти приведет к снижению объема операций ввода-вывода при чтении (логический диск *\<(\>NTDS*) \ операций чтения/с.  Это означает, что решение хранилища может не быть таким же надежным, как при первоначальном вычислении.  К сожалению, все, что более конкретно, чем эта общая инструкция, зависит от нагрузки клиента, и общее руководство не может быть предоставлено.  Лучшим вариантом является настройка размера хранилища после оптимизации ОЗУ.

#### <a name="virtualization-considerations-for-performance"></a>Вопросы виртуализации для повышения производительности

Как и в случае со всеми предыдущими обсуждениями виртуализации, ключ здесь заключается в том, чтобы базовая общая инфраструктура могла поддерживать нагрузку контроллера домена и другие ресурсы, используя базовый общий носитель и все пути к нему. Это верно, если физический контроллер домена использует один и тот же носитель в инфраструктуре SAN, NAS или iSCSI в качестве других серверов или приложений, будь то гостевая служба, использующая сквозной доступ к инфраструктуре сети SAN, NAS или iSCSI, которая использует Базовый носитель или, если гостевой сервер использует VHD-файл, находящийся локально, на общем носителе или в сети SAN, NAS или iSCSI. Цель планирования состоит в том, чтобы убедиться, что базовый носитель поддерживает общую нагрузку всех потребителей.

Кроме того, с точки зрения гостя, так как существуют дополнительные пути кода, которые необходимо обойти, существует влияние на производительность узла для доступа к любому хранилищу. Не удивительно, что при тестировании производительности службы хранилища указывают на то, что виртуализация влияет на пропускную способность, которая является предметом использования процессора хост-системой (см. приложение а. Критерии размера ЦП), которые, очевидно, влияют на ресурсы узла, требуемые гостевыми системами. Это относится к вопросам виртуализации, связанным с требованиями к обработке в виртуализированном сценарии (см. раздел [рекомендации по виртуализации для обработки](#virtualization-considerations-for-processing)).

Сделать это более сложно — существует множество различных вариантов хранения, которые имеют различные последствия для производительности. В качестве надежной оценки при переходе с физического компьютера на виртуальный используйте множитель 1,10, чтобы настроить различные варианты хранения для виртуализированных гостевых виртуальных машин в Hyper-V, таких как транзитное хранилище, адаптер SCSI или интегрированная среда разработки. Изменения, которые необходимо выполнить при передаче между различными сценариями хранилища, не изменяются, если хранилище является локальным, SAN, NAS или iSCSI.

#### <a name="calculation-summary-example"></a>Пример сводки вычислений

Определение объема операций ввода-вывода, необходимых для нормальной работы системы в нормальных условиях:

- LogicalDisk ( *\<диск\>базы данных NTDS*) \ операций передачи в секунду в течение пикового периода 15 минут 
- Чтобы определить объем операций ввода-вывода, необходимый для хранилища, в котором превышена емкость базового хранилища:
  >*Необходимые операции ввода-вывода* : (логический *\<диск\>базы данных NTDS*) \авг &divide; &times; *\<время чтения с диска (с)\>* \ доступ к диску (сек)) LogicalDisk ( *\< Диск\>базы данных NTDS*) \ считывания/с

|Счетчик|Значение|
|-|-|
|Фактическое дисковое LogicalDisk ( *\<диск\>базы данных NTDS*) \авг диск в секунду/перенос|.02 с (20 миллисекунд)|
|Целевой LogicalDisk ( *\<диск\>базы данных NTDS*) \авг диск в секунду/перенос|.01 с|
|Коэффициент изменения в доступных операциях ввода-вывода|0,02 &divide; 0,01 = 2|  
  
|Значение|Значение|
|-|-|
|LogicalDisk ( *\<диск\>базы данных NTDS*) \ передач/с|400|
|Коэффициент изменения в доступных операциях ввода-вывода|2|
|Всего операций ввода-вывода в секунду в течение пикового периода|800|

Чтобы определить скорость, с которой требуется горячий кэш:

- Определите максимально допустимое время для прогрева кэша. Это либо время, необходимое для загрузки всей базы данных с диска, либо сценарии, в которых вся база данных не может быть загружена в ОЗУ. это максимальное время для заполнения ОЗУ.
- Определите размер базы данных, исключая пробелы.  Дополнительные сведения см. в разделе [Оценка хранилища](#evaluating-for-storage).  
- Разделите размер базы данных на 8 КБ; Это будет общий объем IOs, необходимый для загрузки базы данных.
- Разделить общее количество IOs на число секунд в заданном временном кадре.

Обратите внимание, что частота вычисления не будет точна, поскольку ранее загруженные страницы будут исключены, если в ESE не настроен фиксированный размер кэша, а AD DS по умолчанию использует размер кэша переменных.

|Точки данных для накопления|Значения
|-|-|
|Максимально допустимое время для прогрева|10 минут (600 секунд)
|Размер базы данных|2 ГБ|  
  
|Шаг вычисления|Формулы|Результат|
|-|-|-|
|Вычислить размер базы данных на страницах|(2 ГБ &times; 1024 &times; 1024) = *Размер базы данных в КБ*|2 097 152 КБ|
|Вычислить количество страниц в базе данных|2 097 152 КБ &divide; 8 КБ = *количество страниц*|262 144 страниц|
|Вычисление числа операций ввода-вывода, необходимых для полного перетеплого кэша|262 144 страниц &divide; 600 секунд = *требуется операций ввода-вывода*|437 ОПЕРАЦИЙ ВВОДА-ВЫВОДА|

## <a name="processing"></a>Обработка

### <a name="evaluating-active-directory-processor-usage"></a>Оценка использования процессора Active Directory

Для большинства сред после правильной настройки хранилища, ОЗУ и сети, как описано в разделе Планирование, Управление объемом вычислительной мощности будет компонентом, который заслуживает наибольшего внимания. Для оценки производительности ЦП необходимо иметь две сложности:

- Правильно ли работают приложения в среде в инфраструктуре общих служб и рассматриваются в разделе "Отслеживание дорогостоящих и неэффективных поисков" статьи создание более эффективных Microsoft Active Приложения с поддержкой каталогов или переход от более низкоуровневых вызовов SAM к вызовам LDAP.  
  
  В более крупных средах это важно, так как плохо закодированные приложения могут изменчивости нагрузку на ЦП, "украсть" время ЦП из других приложений, искусственно устранить потребности в мощности и равномерно распределить нагрузку на Контроллеры домена.  
- Так как AD DS представляет собой распределенную среду с большим количеством потенциальных клиентов, оценка расходов на "один клиент" является средой в соответствии с шаблонами использования и типом или количеством приложений, использующих AD DS. Вкратце, как и в разделе "сети" для широкого применения, этот подход лучше всего подходит для оценки общей емкости, необходимой в среде.

Для существующих сред, так как размер хранилища был рассмотрен ранее, предполагается, что хранилище имеет правильный размер и, таким образом, данные, касающиеся загрузки процессора, являются допустимыми. Чтобы выполнить итерацию, важно убедиться, что узкое место в системе не является производительностью хранилища. Если существует узкое место, а процессор находится в состоянии ожидания, то в случае удаления узкого места произойдет состояние простоя.  По мере удаления состояний ожидания процессора по определению загрузка ЦП увеличивается, так как больше не требуется ждать данных. Таким путем, собирайте счетчики производительности "логический диск ( *\<диск\>базы данных NTDS*) \авг время чтения с диска (с)\\" и "процесс (LSASS)% загруженности процессора". Данные в "процессе (LSASS)\\% загруженности процессора" будут искусственно низкими, если "логический диск ( *\<диск\>базы данных NTDS*) \авг время чтения с диска (с)" превышает 10 – 15 мс, что является общим пороговым значением, которое служба поддержки Майкрософт использует. для устранения проблем с производительностью, связанными с хранилищем. Как и раньше, рекомендуется использовать интервалы выборки 15, 30 или 60 минут. Что-то меньше, как правило, слишком изменчиво для приемлемых измерений; чем больше, тем более плавное количество просматривается слишком часто.

### <a name="introduction"></a>Введение

Чтобы спланировать планирование ресурсов для контроллеров домена, вычислительная мощность требует наиболее тщательного внимания и понимания. При определении размера систем для обеспечения максимальной производительности всегда существует компонент, который является узким местом, а в контроллере домена с надлежащими размерами это будет процессор.

Как и в разделе «сети», где требование к среде проверяется отдельно для каждого узла, то оно должно быть сделано для требуемой емкости вычислений. В отличие от раздела "сети", где доступные сетевые технологии значительно превышают нормальную потребность, обратите особое внимание на изменение размера ЦП.  Как любая среда даже умеренного размера; что угодно в нескольких тысячах одновременных пользователей может повести значительную нагрузку на ЦП.

К сожалению, из-за огромной вариативности клиентских приложений, использующих AD, Общая оценка пользователей на каждый ЦП прискорбно неприменима ко всем средам. В частности, требования к вычислениям подчиняются поведению пользователя и профилю приложения. Таким образом, каждая среда должна иметь индивидуальное изменение размера.

#### <a name="target-site-behavior-profile"></a>Профиль поведения целевого сайта

Как упоминалось ранее, при планировании емкости для всего сайта цель заключается в проектировании с использованием *N* + 1 емкости, так что сбой одной системы в течение пикового периода позволит продолжить обслуживание с разумным уровнем качества. Это означает, что в сценарии "*N*" нагрузка на все поля должна быть меньше 100% (еще лучше, менее 80%) в периоды пиковой нагрузки.

Кроме того, если приложения и клиенты на сайте используют рекомендации по поиску контроллеров домена (то есть с помощью [функции DsGetDcName](http://msdn.microsoft.com/en-us/library/windows/desktop/ms675983(v=vs.85).aspx)), клиенты должны быть относительно равномерно распределены с небольшими временными пиковыми нагрузками из-за любого число факторов.

В следующем примере выполняются следующие предположения:

- Каждый из пяти контроллеров домена на сайте имеет четыре ЦП.
- Общая Целевая загрузка ЦП в рабочее время составляет 40% при нормальных условиях работы ("*n* + 1") и 60% в противном случае ("*n*"). В нерабочее время целевое использование ЦП составляет 80%, так как предполагается, что программное обеспечение для архивации и другое обслуживание будут потреблять все доступные ресурсы.

![Диаграмма использования ЦП](media/capacity-planning-considerations-cpu-chart.png)

Анализ данных в (служебной программе для процессора с информацией о\% процессоре (_Total)) для каждого контроллера домена:

- В большинстве случаев нагрузка распределяется равномерно, что будет ожидать, когда клиенты используют локатор контроллеров домена и имеют хорошо написанный Поиск. 
- Количество пиковых скачков, равное 10%, составляет 5%, а объем — 20%. В общем случае, если они не вызывают превышение целевого объекта плана загрузки, исследование этих данных не имеет значений.  
- Пиковый период для всех систем составляет от 8:00 AM до 9:15 AM. Благодаря плавному переходу от примерно 5:00 до 5:00 PM это, как правило, говорит о бизнес-цикле. Более случайные всплески загрузки ЦП в сценарии с поблочным переходом между 5:00 PM и 4:00 AM будут за пределами задач планирования емкости.

  >[!NOTE]
  >В хорошо управляемой системе говорят о пиковых нагрузках — это программное обеспечение для архивации, полная проверка антивирусной программы, Инвентаризация оборудования или программного обеспечения, развертывание программного обеспечения или исправлений и т. д. Так как они выходят за пределы рабочего цикла "пиковый пользователь", целевые объекты не превышены.

- Так как каждая система составляет около 40% и все системы имеют одинаковое количество процессоров, один из которых должен быть недоступен или отключен, остальные системы будут выполняться с оценкой 53% (нагрузка System D% 40% равномерно разбивается и добавляется к существующей нагрузке 40% системы A и System C). По ряду причин это линейное допущение не вполне точным, но обеспечивает достаточную точность для датчика.  

  **Альтернативный сценарий —** Два контроллера домена, работающие на 40%: Один контроллер домена завершается неудачно, расчетная загрузка ЦП на остальной будет приблизительно 80%. Это далеко не превышает пороговые значения, указанные выше для плана емкости, и также начинает значительно ограничить объем головного помещения на 10% до 20%, показанный в профиле загрузки выше, что означает, что пики будут выгружать контроллер домена в 90% 100% во время сценария "*N*" и de Конечная скорость реагирования.

### <a name="calculating-cpu-demands"></a>Вычисление требований к ЦП

Счетчик объекта производительности\\"Процесс% загруженности процессора" суммирует общее время, которое все потоки приложения тратят на ЦП и делит на общее количество пройденного системного времени. В результате многопоточное приложение в многопроцессорной системе может превышать 100% времени ЦП и будет интерпретироваться совершенно иначе, чем "информация о\\процессоре% процессора". На практике можно просмотреть "процесс (LSASS\\)% загруженности процессора" как число процессоров, работающих на 100%, которые необходимы для поддержки требований процесса. Значение 200% означает, что 2 процессора, каждый на 100%, необходимы для поддержки полной нагрузки AD DS. Несмотря на то, что ЦП, работающий на 100% емкости, является наиболее экономичным с точки зрения использования ЦП и энергопотребления, по ряду причин, которые подробно описаны в приложении а, более высокая скорость реагирования в многопоточной системе возникает, когда система не работает на 100%.

Для поддержки временных пиковых нагрузок в клиентской нагрузке рекомендуется ориентироваться на пиковый период ЦП в диапазоне от 40% до 60% от емкости системы. Работая с приведенным выше примером, это означает, что для загрузки AD DS (LSASS) потребуется от 3,33 (60% целевого) до 5 (40% целевого) ЦП. Дополнительную емкость следует добавлять в соответствии с потребностями базовой операционной системы и других необходимых агентов (таких как антивирусные программы, резервное копирование, мониторинг и т. д.). Хотя влияние агентов необходимо оценивать для каждого окружения, можно установить оценку в диапазоне от 5% до 10% от одного ЦП. В текущем примере это предложит, что в периоды пиковой нагрузки потребуется от 3,43 (60% целевого) до 5,1 (40% целевого) ЦП.

Самый простой способ сделать это — использовать представление с областями с накоплением в мониторе надежности и производительности Windows (PerfMon), убедившись, что все счетчики масштабируются одинаково.

Допущения:

- Цель состоит в том, чтобы уменьшить объем места до минимально возможного количества серверов. В идеале один сервер будет передавать нагрузку и дополнительный сервер, добавленный для обеспечения избыточности (сценарий *N* + 1).

![Диаграмма "время процессора" для процесса LSASS (для всех процессоров)](media/capacity-planning-considerations-proc-time-chart.png)

Знания, полученные на основе данных в диаграмме (процесс (LSASS\\)% загруженности процессора):

- Рабочий день начинается с 7:00 и уменьшается в 5:00 РМ.
- Пиковая максимальная загруженность — от 9:30 до 11:00 AM. 
  > [!NOTE]
  > Все данные производительности являются историческими. Пиковая точка данных в 9:15 указывает на загрузку с 9:00 до 9:15.
- Существуют пики до 7:00 AM, которые могут указывать на загрузку из разных часовых поясов или фоновых действий инфраструктуры, таких как резервные копии. Так как пиковое значение 9:30 AM превышает это действие, оно не имеет значения.
- На сайте имеется три контроллера домена.

При максимальной нагрузке LSASS потребляет около 485% от одного ЦП или 4,85 ЦП, работающих по адресу 100%. Как и в случае с формулами выше, это означает, что для AD DS требуется сайт примерно 12,25 ЦП. Добавьте приведенные выше предложения от 5% до 10% для фоновых процессов, что означает замену сервера сегодня для поддержки такой же нагрузки потребуется примерно 12,30 процессоров 12,35. Оценка среды для роста теперь должна быть продолжена.

### <a name="when-to-tune-ldap-weights"></a>Время настройки веса LDAP

Существует несколько сценариев, в которых следует учитывать настройку [лдапсрввеигхт](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc957291(v=technet.10)) . В контексте планирования загрузки это было бы сделано, если загрузка приложения или пользователя не сбалансирована, или базовые системы не сбалансированы с точки зрения возможности. Причины, по которым не нужно планировать ресурсы, выходят за рамки этой статьи.

Существуют две распространенные причины для настройки веса LDAP:

- Эмулятор основного контроллера домена — это пример, который влияет на каждую среду, для которой не происходит равномерного распределения нагрузки пользователя или приложения. Так как некоторые средства и действия предназначены для эмулятора основного контроллера домена, например средства управления групповая политика, вторая попытка в случае сбоев проверки подлинности, установки доверия и т. д. ресурсы ЦП эмулятора основного контроллера домена могут быть более требовательны, чем в других местах сайт.
  - Это полезно, только если есть заметная разница в использовании ЦП, чтобы снизить нагрузку на эмулятор основного контроллера домена и повысить нагрузку на другие контроллеры доменов, что обеспечит более равномерное распределение нагрузки.
  - В этом случае задайте Лдапсрввеигхт между 50 и 75 для эмулятора основного контроллера домена.
- Серверы с разными числами ЦП (и скоростями) на сайте.  Например, предположим, что имеется 2 8 основных сервера и 1 4-Core Server.  На последнем сервере имеются половины процессоров двух других серверов.  Это означает, что распределенная нагрузка клиента увеличит среднюю нагрузку на ЦП на основе четырех ядер, чтобы примерно дважды вычислить восемь-ядерные поля.
  - Например, поля 2 8-Core будут работать на 40%, а поле с четырьмя ядрами будет выполняться в 80%.
  - Кроме того, рассмотрим влияние потери 1 8 ядер в этом сценарии, в частности тот факт, что 4-ядерное поле теперь будет перегружено.

#### <a name="example-1---pdc"></a>Пример 1. PDC

| |Использование с по умолчанию|Создать Лдапсрввеигхт|Оценка нового использования|
|-|-|-|-|
|Контроллер домена 1 (эмулятор основного контроллера домена)|53%|57|40 %|
|КОНТРОЛЛЕР ДОМЕНА 2|33%|100|40 %|
|КОНТРОЛЛЕР ДОМЕНА 3|33%|100|40 %|

Здесь следует обратить внимание на то, что если роль эмулятора основного контроллера домена передается или захватывается, особенно на другом контроллере в сайте, новый эмулятор PDC будет значительно увеличен.

Используя пример из [профиля поведения целевого сайта](#target-site-behavior-profile), предполагалось, что все три контроллера домена на сайте имели четыре процессора. Что произойдет в нормальных условиях, если на одном из контроллеров домена имелось восемь ЦП? При использовании на уровне 40% можно было бы использовать два контроллера домена, а на 20% — один. Хотя это и неплохо, существует возможность немного улучшить балансировку нагрузки. Для этого используйте вес LDAP.  Пример сценария:

#### <a name="example-2---differing-cpu-counts"></a>Пример 2. разное число ЦП

| |Программа процессора\\сведений о %&nbsp;процессоре (_Total)<br />Использование с по умолчанию|Создать Лдапсрввеигхт|Оценка нового использования|
|-|-|-|-|
|4 — ЦП, КОНТРОЛЛЕР ДОМЕНА 1|40|100|30 %|
|4 — ЦП, DC 2|40|100|30 %|
|8-ПРОЦЕССОРНЫЙ КОНТРОЛЛЕР ДОМЕНА 3|20|200|30 %|

Тем не менее будьте внимательны при использовании этих сценариев. Как можно увидеть выше, математические знаки выглядят на самом деле хорошо и неплохо. Но в этой статье Планирование сценария "*N* + 1" является важнейшим фактором. Влияние одного контроллера домена на режим «вне сети» должно рассчитываться для каждого сценария. В приведенном выше сценарии, где распределение нагрузки выполняется даже для обеспечения загрузки 60% во время сценария "*N*" с равномерной балансировкой нагрузки на всех серверах, распределение будет хорошо, так как коэффициенты остаются согласованными. При просмотре сценария настройки эмулятора основного контроллера домена и в общем случае, когда нагрузка пользователя или приложения не сбалансирована, результат очень отличается.

| |Настроенное использование|Создать Лдапсрввеигхт|Оценка нового использования|
|-|-|-|-|
|Контроллер домена 1 (эмулятор основного контроллера домена)|40 %|85|47%|
|КОНТРОЛЛЕР ДОМЕНА 2|40 %|100|53%|
|КОНТРОЛЛЕР ДОМЕНА 3|40 %|100|53%|

### <a name="virtualization-considerations-for-processing"></a>Рекомендации по виртуализации для обработки

Существует два уровня планирования емкости, которые необходимо выполнить в виртуализованной среде. На уровне узла аналогично идентификации бизнес-цикла, описанного ранее для обработки контроллера домена, необходимо определить пороговые значения в течение пикового периода. Так как базовые субъекты одинаковы для планирования машинных гостевых потоков на ЦП, как для получения AD DSных потоков на ЦП на физическом компьютере, рекомендуется использовать ту же цель 40% на 60% на базовом узле. На следующем уровне гостевой уровень, так как участники планирования потоков не изменились, цель в гостевой системе остается в диапазоне от 40% до 60%.

В сценарии с прямым сопоставлением одним гостем на каждом узле необходимо добавить все планирование загрузки на этот момент к требованиям (ОЗУ, диск, сеть) базовой операционной системы узла. В сценарии с общим узлом тестирование указывает на то, что на производительность базовых процессоров влияет на 10%. Это означает, что если для сайта требуется 10 ЦП на целевом объекте 40%, рекомендуемый объем виртуальных ЦП, выделяемый всем "*N*" гостевым виртуальным машинам, будет равен 11. На сайте с смешанным распределением физических серверов и виртуальных серверов модификатор применяется только к виртуальным машинам. Например, если у сайта есть сценарий "*N* + 1", один физический или прямой сопоставленный сервер с 10 процессорами будет примерно соответствовать одному гостевому серверу с 11 процессорами на узле, при этом 11 ЦП зарезервированы для контроллера домена.

В ходе анализа и вычисления количества ЦП, необходимого для поддержки AD DS нагрузки, количество процессоров, которые сопоставляются с тем, что может быть приобретено в терминах физического оборудования, не всегда сопоставляется аккуратно. Виртуализация устраняет необходимость в округлении. Виртуализация сокращает усилия, необходимые для добавления вычислительной мощности на сайт, с учетом простоты добавления ЦП в виртуальную машину. Он не устраняет необходимость в точном вычислении мощности вычислений, чтобы базовое оборудование было доступно при добавлении дополнительных процессоров в гости.  Как всегда, не забывайте планировать и отслеживать рост спроса.

### <a name="calculation-summary-example"></a>Пример сводки вычислений

|Система|Пиковая загрузка ЦП|
|-|-|-|
|КОНТРОЛЛЕР ДОМЕНА 1|120%|
|КОНТРОЛЛЕР ДОМЕНА 2|147%|
|Контроллер домена 3|218%|
|Всего используемых ЦП|485%|  
  
|Число целевых систем|Общая пропускная способность (из выше)|
|-|-|
|ЦП, необходимые для целевого объекта на 40%|4,85 &divide; . 4 = 12,25|

Повторяя из-за важности этого момента, не *забывайте планировать рост*. Предполагая, что рост на 50% в течение следующих трех лет, в этой среде потребуется 18,375 &times; процессоров (12,25 1,5) с пометкой за три года. Альтернативный план будет рассмотрен после первого года и добавлять дополнительную емкость по мере необходимости.

### <a name="cross-trust-client-authentication-load-for-ntlm"></a>Загрузка проверки подлинности клиента с перекрестным доверием для NTLM

#### <a name="evaluating-cross-trust-client-authentication-load"></a>Оценка загрузки проверки подлинности клиента с перекрестным доверием

Многие среды могут иметь один или несколько доменов, Соединенных отношением доверия. Запрос на проверку подлинности для удостоверения в другом домене, который не использует проверку подлинности Kerberos, должен пройти от доверия с помощью безопасного канала контроллера домена к другому контроллеру домена в конечном домене или в следующем домене в пути. в конечный домен. Количество одновременных вызовов с помощью безопасного канала, который контроллер домена может сделать контроллером домена в доверенном домене, управляется параметром, известным как **MaxConcurrentApi**. Для контроллеров домена обеспечение того, что безопасный канал может справиться с нагрузкой, достигается одним из двух подходов: Настройка **MaxConcurrentApi** или, в лесу, создание сокращенных доверий. Чтобы оценить объем трафика в отдельном доверии, ознакомьтесь со статьей Настройка [производительности для проверки подлинности NTLM с помощью параметра MaxConcurrentApi](https://support.microsoft.com/kb/2688798).

Во время сбора данных это, как и в случае с другими сценариями, должно быть собрано в периоды пиковой занятости, чтобы данные были полезными.

> [!NOTE]
> Сценарии с подразделениями и между лесами могут привести к тому, что проверка подлинности может пройти несколько доверий и необходимо настроить каждый этап.

#### <a name="planning"></a>Планирование

Существует несколько приложений, которые используют проверку подлинности NTLM по умолчанию или используют ее в определенном сценарии конфигурации. Серверы приложений увеличивают емкость и обслуживают увеличивающееся количество активных клиентов. Кроме того, существует тенденция, с которой клиенты открывают сеансы в течение ограниченного времени, а затем регулярно подключаются к ним (например, по запросу на синхронизацию по электронной почте). Еще один распространенный пример для высокой нагрузки NTLM — серверы веб-прокси, требующие проверки подлинности для доступа к Интернету.

Эти приложения могут вызвать значительную нагрузку при проверке подлинности NTLM, что может привести к значительной нагрузке на контроллеры домена, особенно если пользователи и ресурсы находятся в разных доменах.

Существует несколько подходов к управлению нагрузкой с перекрестным доверием, которая в практике используется совместно, а не в эксклюзивном сценарии или. Возможны следующие варианты:

- Сокращение проверки подлинности клиента с перекрестным доверием путем обнаружения служб, которые пользователь использует в том же домене, в котором находится пользователь.
- Увеличьте число доступных защищенных каналов. Это относится ко всем контейнерам и трафику между лесами и называется сокращенным доверием.
- Настройте параметры по умолчанию для **MaxConcurrentApi**.

Для настройки **MaxConcurrentApi** на существующем сервере используется уравнение:

> *New_MaxConcurrentApi_setting* &ge; (*semaphore_acquires*  +  *semaphore_time-outs*) &times; *average_semaphore_hold_time* &divide; *time_collection_length*

Дополнительные сведения см [. в статье базы знаний 2688798: Инструкции по настройке производительности для проверки подлинности NTLM с помощью параметра](http://support.microsoft.com/kb/2688798)MaxConcurrentApi.

## <a name="virtualization-considerations"></a>Сведения о виртуализации

Нет, это параметр настройки операционной системы.

### <a name="calculation-summary-example"></a>Пример сводки вычислений

|Тип данных|Значение|
|-|-|
|Количество получений семафора (минимум)|6 161|
|Количество получений семафора (максимум)|6 762|
|Время ожидания семафора|0|
|Среднее время удержания семафора|0,012|
|Длительность сбора (в секундах)|1:11 минут (71 секунд)|
|Формула (из KB 2688798)|((6762 &ndash; 6161) + 0) &times; 0,012/|
|Минимальное значение для **MaxConcurrentApi**|((6762 &ndash; 6161) + 0) &times; 0,012 &divide; 71 =. 101|

Для этой системы в течение этого периода времени допустимы значения по умолчанию.

## <a name="monitoring-for-compliance-with-capacity-planning-goals"></a>Мониторинг соответствия требованиям с целями планирования ресурсов

В этой статье рассматривается планирование и масштабирование для достижения целей использования. Ниже приведена сводная диаграмма рекомендуемых пороговых значений, которые необходимо отслеживать, чтобы убедиться, что системы работают в пределах соответствующих пороговых значений емкости. Помните, что эти значения не являются пороговыми значениями производительности, а пороговые значения планирования емкости. Сервер, работающий за пределами этих порогов, будет работать, но пора начать проверку правильности работы всех приложений. Если говорят, что приложения хорошо работают, то пора начать оценку обновлений оборудования или других изменений конфигурации.

|Category|Счетчик производительности|Интервал и выборка|Целевой объект|Предупреждение|
|-|-|-|-|-|
|Процессор|Процессорная информация (_Total\\)% утилита процессора|60 мин|40 %|60 %|
|ОЗУ (Windows Server 2008 R2 или более ранней версии)|Память \ доступно МБ|< 100 МБ|Н/Д|< 100 МБ|
|ОЗУ (Windows Server 2012)|Среднее время существования резервного кэша Мемори\лонг-терм|30 мин|Необходимо проверить|Необходимо проверить|
|Network|Сетевой интерфейс (\*) отправлено \/с<br /><br />Сетевой интерфейс (\*) получено \/с|30 мин|40 %|60 %|
|Служба хранилища|LogicalDisk ( *\<диск\>базы данных NTDS*) \авг диск сек/чтение<br /><br />LogicalDisk ( *\<диск\>базы данных NTDS*) \авг диск сек/запись|60 мин|10 мс|15 мс|
|Службы AD|Время хранения\*семафора \авераже Netlogon ()|60 мин|0|1 с|

## <a name="appendix-a-cpu-sizing-criteria"></a>Приложение A. Критерии размера ЦП

### <a name="definitions"></a>Определения

**Процессор (микропроцессор) —** компонент, считывающий и выполняющий инструкции программы  

**ЦП —** Центральная единица обработки  

**Многоядерный процессор —** несколько процессоров в одном интегрированном канале  

Несколько **ЦП —** множественные ЦП, а не в одном интегрированном канале  

**Логический процессор —** одна логическая вычислительная система с точки зрения операционной системы.  

Это включает в себя технологию с поддержкой технологии Hyper-Threading, одну ядро на многоядерном процессоре или один базовый процессор.  

Так как современные серверные системы имеют несколько процессоров, несколько многоядерных процессоров и технологию Hyper-Threading, эти сведения обобщены в обоих сценариях. Таким образом, термин логический процессор будет использоваться, так как он представляет собой операционную систему и приложение доступных вычислительных ядер.

### <a name="thread-level-parallelism"></a>Параллелизм на уровне потока

Каждый поток является независимой задачей, так как каждый поток имеет собственный стек и инструкции. Так как AD DS является многопоточным и количество доступных потоков можно настроить с помощью [инструкции по просмотру и настройке политики LDAP в Active Directory с помощью программы Ntdsutil. exe](http://support.microsoft.com/kb/315071), она хорошо масштабируется по нескольким логическим процессорам.

### <a name="data-level-parallelism"></a>Параллелизм на уровне данных

Это включает в себя общий доступ к данным в нескольких потоках в одном процессе (в случае отдельного процесса AD DS) и в нескольких потоках в нескольких процессах (в общем). Что касается более простого случая, это означает, что любые изменения данных отражаются во всех работающих потоках на всех уровнях кэша (L1, L2 и L3) во всех ядрах, где выполняются потоки, а также для обновления общей памяти. Производительность может снизиться во время операций записи, в то время как все различные расположения памяти будут согласованы до того, как обработка инструкций будет продолжена.

### <a name="cpu-speed-vs-multiple-core-considerations"></a>Требования к производительности ЦП и нескольким ядрами

Общее правило "бегунок" — это более быстрые логические процессоры. Сократите продолжительность обработки серии инструкций, в то время как большее число логических процессоров означает, что другие задачи могут выполняться одновременно. Эти правила прокрутки вниз по мере того, как сценарии становятся более сложными с учетом выборки данных из общей памяти, ожидания параллелизма на уровне данных и издержек, связанных с управлением несколькими потоками. Это также объясняется тем, что масштабируемость в многоядерных системах не является линейной.

Примите во внимание следующие аналоги: Представьте себе, что каждый поток является отдельным автомобилем, каждая из них является ядром, а максимальная скорость — тактовой частотой.

1. Если на автомобиль приходится только один автомобиль, не имеет значения, есть ли два желоба или 12 дорожек. Этот автомобиль будет работать так быстро, как допустима предельная скорость.
1. Предположим, что данные, необходимые потоку, недоступны сразу же. Аналогом было бы то, что сегмент дороги будет выключен. Если на автомобиль приходится только один автомобиль, то не имеет значения, какое ограничение скорости происходит до повторного открытия полосы (данные извлекаться из памяти).
1. По мере роста количества автомобилей затраты на управление количеством автомобилей увеличиваются. Сравните опыт и количество, необходимое, если интенсивность практически пуста (например, позднее вечером), а также когда трафик является большим (например, с середины до полуночи, но не с часами). Кроме того, обратите внимание на количество внимания, необходимое при работе с двумя полосами, в которых есть только одна полоса, чтобы беспокоиться о том, что делают драйверы, а также об шести дисках, где приходится беспокоиться о том, что делают многие другие драйверы.
   > [!NOTE]
   > В следующем разделе приведено поэтапное описание сценария с почасовым часовым циклом. Время отклика/степень занятости системы влияет на производительность.

В результате, особенности, связанные с более или более быстрыми процессорами, становятся очень субъективными для поведения приложений, что в случае AD DS является очень специфичным для среды и даже отличается от сервера к серверу в среде. Именно поэтому ссылки, приведенные выше в статье, не вкладываются в чрезмерную точность, а в вычислениях включается поле безопасности. При принятии решений о покупке, основанных на бюджете, рекомендуется сначала оптимизировать использование процессоров на 40% (или желаемый номер для среды), прежде чем приступить к покупке более быстрых процессоров. Повышенная синхронизация для большего числа процессоров снижает истинное преимущество дополнительных процессоров от линейной прогрессии&times; (2 число процессоров обеспечивает менее 2&times; доступных дополнительных вычислений).

> [!NOTE]
> Законы закона амдахл и Густафсон являются важными концепциями.

### <a name="response-timehow-the-system-busyness-impacts-performance"></a>Время ответа/влияние доступности системы на производительность

Теория очереди — это математическое исследование ожидающих строк (очередей). В теории очередей использование закона представляется уравнением:

*U* k = *B* &divide; *T*

Где *U* k — процент использования, *B* — это количество времени занятости, а *T* — общее время обнаружения системы. В контексте Windows это означает число потоковых потоков 100-наносекундных (NS), которые находятся в состоянии выполнения, деленное на количество 100-NS интервалов, которые были доступны в течение заданного интервала времени. Это именно формула для вычисления% Processor Utility (объект ссылочного [процессора](https://docs.microsoft.com/previous-versions/ms804036(v=msdn.10)) и [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10))).

Теоретически в очереди также представлена формула: *N*  =  *u* k &divide; (1 &ndash; *U* k) для оценки количества ожидающих элементов на основе использования (*N* — длина очереди). Построение диаграммы за все интервалы использования дает следующие оценки, определяющие, как долго очередь должна получить процессор в любой заданной нагрузке ЦП.

![Длина очереди](media/capacity-planning-considerations-queue-length.png)

Это наблюдалось, что после 50% загрузки ЦП в среднем всегда существует время ожидания одного другого элемента в очереди, что значительно увеличится после примерно 70% использования ЦП.

Возврат к управляющей аналоговой, используемой ранее в этом разделе:

- Время, занятое "середина-вечера", гипотетически, будет находиться где-то в диапазоне 40% до 70%. Достаточно трафика, чтобы одна возможность выбрать любую полосу не ограничена главным образом, и вероятность того, что другой драйвер в таком случае, в высоком масштабе, не требует «поиска» безопасного промежутка между другими автомобилями в дороге.
- Обратите внимание, что по мере того, как трафик приближается к одному часу, система дороги приближается к 100% емкости. Изменение желобов может стать очень сложной задачей, так как автомобили настолько близки, что для этого необходимо соблюдать осторожность.

Именно поэтому долгосрочные средние значения емкости в 40% позволяют использовать головное место для аномальных пиковых нагрузок в нагрузке, независимо от того, наблюдается ли транзитный (например, плохо закодированные запросы, выполняемые в течение нескольких минут) или аномальные нагрузки при общей нагрузке (утром первый день после длинного выходного дня).

Приведенная выше инструкция относится к вычислению "% загруженности процессора" так же, как и при использовании закона общего читателя. Для более математических требований:  
- Перевод [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10))
  - *B* = число периодов бездействия в 100-нс, потраченных на логический процессор. Изменение в переменной "*X*" в вычислении [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10))
  - *T* = общее число интервалов 100-НС в заданном диапазоне времени. Изменение в переменной "*Y*" в вычислении [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10)) .
  - *U* k = процент использования логического процессора на время простоя потока или% времени простоя.  
- Выработка математических функций:
  - *U* k = 1 –% загруженности процессора
  - % Загруженности процессора = 1 – *U* k
  - % Загруженности процессора = 1 – *B* / *T*
  - % Загруженности процессора = 1 – *x1* – *x0* / *Y1* — *Y0*

### <a name="applying-the-concepts-to-capacity-planning"></a>Применение концепций к планированию ресурсов

Приведенные выше математические вычисления могут определить количество логических процессоров, необходимых для системы, кажутся слишком сложными. Именно поэтому подход к выбору размера систем ориентирован на определение максимального использования целевых объектов на основе текущей нагрузки и вычисление числа логических процессоров, необходимых для их получения. Кроме того, хотя скорость логических процессоров окажет значительное влияние на производительность, эффективность кэширования, требования к согласованию памяти, планирование потоков и синхронизацию, а также некачественная балансировка нагрузки клиентов, все они оказывают значительное влияние на производительность, которая будет изменяться отдельно для каждого сервера. Благодаря относительно дешевой стоимости вычислений, попытка проанализировать и определить необходимое количество процессоров станет более академическим упражнением по сравнению с обеспечением ценности бизнеса.

40% не является жестким и быстрым требованием. это разумный запуск. Различным потребителям Active Directory требуются различные уровни реагирования. Возможны ситуации, когда среды могут работать на уровне 80% или 90% как устойчивое среднее, так как увеличение времени ожидания доступа к процессору заметно не повлияет на производительность клиента. Очень важно перебрать, что в системе много областей, намного медленнее, чем логический процессор в системе, включая доступ к ОЗУ, доступ к диску и передачу ответа по сети. Все эти элементы должны быть настроены в сочетании. Примеры:

- Добавление процессоров в систему под управлением 90% с привязкой к диску, вероятно, не будет значительно повысить производительность. Более глубокий анализ системы, скорее всего, определит наличие большого количества потоков, которые даже не поступают на процессор, поскольку они ожидают завершения операций ввода-вывода.
- Устранение проблем, связанных с диском, потенциально означает, что потоки, которые ранее затратили много времени в состоянии ожидания, больше не будут находиться в состоянии ожидания для ввода-вывода и будут больше конкуренции за время ЦП, что означает использование 90% в предыдущем в примере выйдет 100% (так как это не может быть выше). Оба компонента должны быть настроены совместно.
  > [!NOTE]
  > Процессорная информация (*\\)% процессорная программа может превышать 100% систем с режимом "Turbo".  Именно в этом случае ЦП превышает номинальную тактовую частоту процессора в течение коротких периодов.  Документация по изготовителям ЦП и описание счетчика для получения более подробной информации.  

Обсуждение всех рекомендаций по использованию системы также приводит к контроллерам домена беседы в качестве виртуализованных гостей. [Время отклика и влияние доступности системы на производительность](#response-timehow-the-system-busyness-impacts-performance) в виртуализированном сценарии относятся как к узлу, так и к гостевой системе. Именно поэтому в узле с одним гостевым узлом контроллер домена (и, как правило, любая система) практически не отличается от производительности физического оборудования. Добавление дополнительных гостей на узлы увеличивает использование базового узла, тем самым увеличивая время ожидания для получения доступа к процессорам, как описано выше. В небольшой степени использование логического процессора должно осуществляться как на узле, так и на уровне гостя.

При расширении предыдущих аналогов в качестве физического оборудования Гостевая виртуальная машина будет аналогова с помощью шины (Экспресс-шина, которая переходит непосредственно к назначению, которое Rider требуется). Представьте себе следующие четыре сценария:

- Она находится в нерабочее время, Rider на шине, которая почти пуста, а шина находится в дороге, которая также почти пуста. Так как нет трафика, с которым приходится конкурировать, Rider легко передается и получается так же быстро, как если бы Rider был управляемым. Время поездки Rider по-прежнему ограничивается ограничением скорости.
- Это время отключается, поэтому шина почти пуста, но большая часть дорожек в дороге закрывается, поэтому автомобиль по-прежнему перегружен. Rider находится на почти пустой шине в перегруженной дороги. Несмотря на то, что Rider не имеет большого количества конкуренции на шине, где размещается, общее время поездки по-прежнему определяется остальной частью трафика, находящегося за пределами.
- Это происходит в течение часа, поэтому перегрузка и шина перегружаются. Это не только происходит дольше, но прием и отключение шины является кошмаром, так как люди нанимают налево и не намного лучше. Добавление дополнительных шин (логических процессоров к гостевой системе) не означает, что они могут помещаться в дороге более просто, или что поездку будет сокращена.
- Окончательный сценарий, хотя он может растягивать аналогию немного, — место, где шина заполнена, но дорожка не перегружена. Хотя у Rider по-прежнему возникают проблемы с включением и отключением шины, путешествие будет эффективным после того, как шина находится в дороге. Это единственный сценарий, в котором Добавление дополнительных шин (логических процессоров к гостевой системе) повысит производительность гостевых систем.

В этом случае относительно легко выполнить экстраполяцию, что между 0%-использовано и 100-процентным состоянием в дороге и 0%-и 100-процентным состоянием шины с различной степенью влияния.

Применение участников выше 40% ЦП в качестве разумного целевого объекта для узла, а также в том, что гостевой сервер является разумным запуском по той же причине, что и выше, то есть объем постановки в очередь.

## <a name="appendix-b-considerations-regarding-different-processor-speeds-and-the-effect-of-processor-power-management-on-processor-speeds"></a>Приложение B. Рекомендации по различным тактам процессора и результату управления питанием процессора на скорости процессора

В разделах, посвященных выбору процессоров, предполагается, что процессор работает с тактовой частотой 100% от тактовой частоты всего времени сбора данных и того, что в системах замены будут использоваться одинаковые процессоры Speed. Несмотря на то, что предполагается, что оба предположения имеют значение false, особенно в Windows Server 2008 R2 и более поздних **версиях**, где схема управления питанием по умолчанию сбалансирована, методология по-прежнему является консервативным подходом. Хотя возможная частота ошибок может увеличиться, увеличение уровня безопасности повышается только при увеличении скорости процессора.

- Например, в сценарии, где требуется 11,25 ЦП, если процессоры работали с половинной скоростью при сборе данных, более точная оценка может составлять 5,125 &divide; 2.
- Невозможно гарантировать, что удвоение скоростей времени будет вдвое больше объема обработки, выполняемой за определенный период времени. Это связано с тем, что время, затрачиваемое процессорами на ожидание ОЗУ или других системных компонентов, может остаться прежним. Общий результат заключается в том, что более быстрые процессоры могут тратить больше времени на время простоя при ожидании выборки данных. Опять же, рекомендуется придерживаться наименьшего общего знаменателя, быть консервативным и не пытаться вычислить потенциально ложный уровень точности, предполагая Линейное сравнение между тактовой частотой процессора.

Кроме того, если скорость процессора в заменяющем оборудовании меньше, чем текущее оборудование, можно было бы повысить оценку процессоров, необходимых для пропорционального объема. Например, вычисляется 10 процессоров для поддержания нагрузки на сайте, а текущие процессоры работают с тактовой частотой 3,3 ГГц, а процессор-замена — с частотой 2,6 ГГц. это снижение скорости на 21%. В этом случае рекомендуемым объемом является 12 процессоров.

С другой стороны, Эта вариативность не изменит целевые показатели использования процессора для управления емкостью. Так как тактовые частоты процессора будут скорректированы динамически в зависимости от требуемой нагрузки, запуск системы при более высоких нагрузках приведет к созданию сценария, в котором ЦП тратит больше времени на более высокую тактовую частоту, что обеспечит максимальную цель до 40% на 100%. состояние тактовой частоты в пиковой нагрузке. Что-то меньше, будет создавать энергосбережение, так как скорость ЦП будет регулироваться при непиковых условиях.

> [!NOTE]
> В процессе сбора данных может потребоваться отключить управление питанием на процессорах (установка **высокой производительности**для схемы управления питанием). Это позволит получить более точное представление о потреблении ресурсов ЦП на целевом сервере.

Чтобы настроить оценки для разных процессоров, они использовались для защиты, исключая другие узкие места, описанные выше, чтобы предположить, что удвоение скорости процессора вдвое превышает объем обработки, который может быть выполнен.  На сегодняшний день внутренняя архитектура процессоров достаточно различается между процессорами, что более безопасный способ оценить влияние использования разных процессоров, чем были взяты данные, — использовать SPECint_rate2006ный тест производительности из стандартного вычисления Корпорация.

1. Найдите оценки SPECint_rate2006 для используемого процессора и планируемого использования.
    1. На веб-сайте оценки производительности "Стандартный" выберите **результаты**, выделите **CPU2006**и выберите **Искать все SPECint_rate2006 результаты**.
    1. В разделе **простой запрос**введите условия поиска для целевого процессора, например **соответствие процессоров 2630 (Баселинетаржет)** и **соответствие процессору "2650" (базовый план)** .
    1. Найдите используемый сервер и конфигурацию процессора (или что-либо близкое, если точное соответствие недоступно) и запишите значение в столбцах **result** и **# Cores** .
1. Чтобы определить модификатор, используйте следующее уравнение:
   >((*Значение оценки для целевой платформы на*ядро) &times; (число*МГц на ядро базовой платформы* &divide; )) ((*значение оценки по базовому значению на*ядро) &times; (число*МГц на ядро целевой платформы*))  

    Используя приведенный выше пример:
   >(35,83 &times; 2000) &divide; (33,75 &times; 2300) = 0,92
1. Умножьте предполагаемое количество процессоров на модификатор.  В приведенном выше примере, чтобы переключиться с процессора 2650-2630 на процессор с частотой + +, &times; необходимо умножить вычисленные процессоры 11,25 0,92 = 10,35.

## <a name="appendix-c-fundamentals-regarding-the-operating-system-interacting-with-storage"></a>Приложение В. Основные сведения об операционной системе, взаимодействующей с хранилищем

Понятия теории очередей, описанные в разделе [время отклика/влияние того, как загруженность системы влияет на производительность](#response-timehow-the-system-busyness-impacts-performance) , также применимы к хранилищу. Знание того, как операционная система обрабатывает операции ввода-вывода, необходимо для применения этих концепций. В операционной системе Microsoft Windows очередь для хранения запросов ввода-вывода создается для каждого физического диска. Однако необходимо выполнить уточнение на физическом диске. Контроллеры и San массивов представляют собой статистические схемы шпинделей в операционную систему в виде отдельных физических дисков. Кроме того, контроллеры и сети SAN могут объединять несколько дисков в один набор массивов, а затем разбивать этот массив на несколько "секций", который, в свою очередь, представлен операционной системе как несколько физических дисков (ref. рис.).

![Блочные диски](media/capacity-planning-considerations-block-spindles.png)  

На этом рисунке два вращения зеркально отражаются и разбиваются на логические области для хранения данных (Data 1 и Data 2). Эти логические области просматриваются операционной системой как отдельные физические диски.

Хотя это может быть очень запутанным, для обнаружения различных сущностей в этом приложении используется следующая терминология:

- **Шпиндел —** устройство, физически установленное на сервере.
- **Array —** коллекция шпинделей, агрегированная по контроллеру.
- **Секция массива —** секционирование агрегированного массива
- **LUN —** массив, используемый при ссылке на San.
- **Диск —** То, что операционная система обнаруживает как один физический диск.
- **Partition —** логическое секционирование, которое операционная система воспринимает как физический диск.

### <a name="operating-system-architecture-considerations"></a>Рекомендации по архитектуре операционной системы

Операционная система создает очередь ввода-вывода с первым входом (FIFO) для каждого наблюдаемого диска. Этот диск может представлять собой дисковод, массив или секцию массива. С точки зрения операционной системы, в отношении обработки операций ввода-вывода, чем больше активных очередей, тем лучше. Так как очередь FIFO сериализуется, это означает, что все операции ввода-вывода, выданные подсистеме хранения, должны обрабатываться в порядке поступления запроса. При корреляции каждого диска, обнаруженного операционной системой с диском или массивом, операционная система теперь поддерживает очередь ввода-вывода для каждого уникального набора дисков, тем самым устраняя состязание за неограниченные ресурсы ввода-вывода на дисках и изолируя требование ввода-вывода к одному свободного. В качестве исключения в Windows Server 2008 введена концепция приоритетов ввода-вывода, и приложения, предназначенные для использования "низкого" приоритета, выходят из этого обычного порядка и принимают на себя рабочее место. Приложения, не запрограммированные для использования «низкого» приоритета по умолчанию — «нормальный».

### <a name="introducing-simple-storage-subsystems"></a>Введение в простые подсистемы хранения данных

Начиная с простого примера (один жесткий диск внутри компьютера) будет определен анализ компонента по компоненту. Разбивая их на основные компоненты подсистемы хранения, система состоит из следующих компонентов:

- **1 –** 10 000 об/мин Ultra Fast SCSI HD (высокая скорость передачи — 20 МБ/с)
- **1 —** Шина SCSI (кабель)
- **1 —** Адаптер Ultra Fast SCSI
- **1 —** 32-разрядная шина PCI с частотой 33 МГц

После определения компонентов можно вычислить, какой объем данных может переносить систему или сколько операций ввода-вывода можно обработать. Обратите внимание, что количество операций ввода-вывода и количество данных, которые могут перетранзитировать систему, коррелированы, но не совпадают. Эта корреляция зависит от того, является ли дисковый ввод-вывод случайным или последовательным, а также размер блока. (Все данные записываются на диск как блок, но разные приложения используют разные размеры блоков.) В зависимости от компонента:

- **Жесткий диск —** На среднем жестком диске 10 000-RPM имеется время поиска в течение 7 мс (МС) и 3 МС времени доступа. Время поиска — это среднее время, необходимое для перемещения головки чтения и записи в расположение на Негнущийся. Время доступа — это среднее время, затрачиваемое на чтение или запись данных на диск, если головной элемент находится в правильном расположении. Таким же средним временем считывания уникального блока данных на уровне 10 000-RPM HD является поиск и доступ к общему объему приблизительно 10 мс (или .010 секунд) на блок данных.

  Если каждый доступ к диску требует перемещения головного компьютера в новое место на диске, поведение чтения и записи называется "случайным". Таким образом, когда все операции ввода-вывода имеют произвольную частоту, 10 000-м в секунду может обслуживать примерно 100 операций ввода-вывода в минуту (в каждой из формул — 1000 мс в секунду на 10 мс на операцию ввода-вывода или 1000/10 = 100 операций ввода/вывода).

  Кроме того, когда все операции ввода-вывода происходят из смежных секторов HD, это называется последовательным вводом-выводом. Последовательный ввод-вывод не имеет времени поиска, так как после завершения первого ввода-вывода заголовок для чтения и записи находится в начале, где хранится следующий блок данных на HD. Таким же размером, 10 000 ГБ, может обрабатывать примерно 333 операций ввода-вывода в секунду (1000 мс в секунду на 3 мс на операцию ввода-вывода).

  >[!NOTE]
  >Этот пример не отражает кэш диска, где обычно хранятся данные одного цилиндра. В этом случае при первом вводе-выводе требуется 10 мс, и диск считывает весь цилиндр. Все остальные последовательные операции ввода-вывода удовлетворяются в кэше. В результате кэширование на диске может повысить производительность последовательного ввода-вывода.
  
  До сих пор скорость передачи жесткого диска была несущественна. Независимо от того, имеет ли жесткий диск 20 МБ/с в ширину или Ultra3 160 МБ/с, фактический объем операций ввода-вывода в секунду, который может обработать 10 000-RPM HD, составляет ~ 100 Random или ~ 300 последовательный ввод/вывод. При изменении размеров блоков в зависимости от приложения, записывающего данные на диск, объем данных, извлекаемых в ходе ввода-вывода, отличается. Например, если размер блока равен 8 КБ, 100 операций ввода-вывода будет считывать или записывать данные на жесткий диск (всего 800 КБ). Однако если размер блока равен 32 КБ, 100 ввода-вывода будет считывать и записывать 3 200 КБ (3,2 МБ) на жесткий диск. При условии, что скорость передачи SCSI превышает общий объем передаваемых данных, получение "ускоренной" скорости передачи не даст ничего. Для сравнения см. следующие таблицы.

  | |7200 RPM 9ms, 4 мс доступ|10 000 RPM 7ms, 3 МС доступ|15 000 RPM 4 мс, 2 мс доступ
  |-|-|-|-|
  |Случайные операции ввода-вывода|80|100|150|
  |Последовательный ввод-вывод|250|300|500|  
  
  |диск 10 000 RPM|Размер блока размером 8 КБ (Active Directory Jet)|
  |-|-|
  |Случайные операции ввода-вывода|800 КБ/с|
  |Последовательный ввод-вывод|2400 Кб/с|

- **Объединительная плата SCSI (шина) —** Понимание того, как «задняя панель SCSI (шина)» или в этом сценарии шлейф, влияет на пропускную способность подсистемы хранения данных, зависит от размера блока. По сути, это вопрос, сколько операций ввода-вывода может обрабатывать шина, если операция ввода-вывода находится в блоках по 8 КБ? В этом сценарии шина SCSI имеет значение 20 МБ/с или 20480 КБ/с. 20480 КБ/с, деленная на 8 КБ, дает максимум примерно 2500 операций ввода-вывода, поддерживаемых шиной SCSI.

  >[!NOTE]
  >Примеры представлены в следующей таблице. В настоящее время большинство подключенных устройств хранения используют PCI Express, что обеспечивает более высокую пропускную способность.  
  
  |Ввод-вывод, поддерживаемый шиной SCSI на размер блока|Размер блока 2 КБ|8 КБ размер блока (AD Jet) (SQL Server 7.0/SQL Server 2000)
  |-|-|-|
  |20 МБ/с|10 000|2 500|
  |40 МБ/с|20 000|5 000|
  |128 МБ/с|65 536|16 384|
  |320 МБ/с|160 000|40 000|

  Как можно определить из этой диаграммы, в представленном сценарии независимо от того, что используется, шина никогда не является узким местом, так как максимальное число дисков — 100 операций ввода-вывода, а не все перечисленные выше пороговые значения.

  >[!NOTE]
  >При этом предполагается, что шина SCSI работает на 100% эффективно.
  
- **Адаптер SCSI —** Для определения объема операций ввода-вывода, которые можно обрабатывать, необходимо проверить спецификации производителя. Для направления запросов ввода-вывода на соответствующее устройство требуется обработка некоторой сортировки, поэтому объем операций ввода-вывода, которые могут быть обработаны, зависит от процессора SCSI-адаптера (или контроллера массива).

  В этом примере предполагается, что будет выполнена обработка 1 000 операций ввода-вывода.

- **Шина PCI —** Это часто наподобный компонент. В этом примере это не будет узким местом. Однако при увеличении масштаба системы это может стать узким местом. Для справки 32-разрядная шина PCI, работающая на 33Mhz, может передавать данные 133 МБ/с. Ниже приведено уравнение.  
  > 32 бит &divide; 8 бит на байт &times; 33 МГц = 133 МБ/с.  

  Обратите внимание, что является теоретическим ограничением; в реальности фактически достигается только около 50% максимума, хотя в некоторых сценариях нагрузки можно получить 75% эффективности для коротких периодов.

  64 66-битная шина PCI поддерживает теоретический максимум (64 бит &divide; 8 бит на байт &times; 66 МГц) = 528 МБ/с. Кроме того, любое другое устройство (например, сетевой адаптер, второй контроллер SCSI и т. д.) снизит доступную пропускную способность. так как пропускная способность является общей, и устройства будут конкурировать за ограниченные ресурсы.

После анализа компонентов этой подсистемы хранения диск является ограничивающим фактором объема операций ввода-вывода, которые могут быть запрошены, и, следовательно, объема данных, которые могут переносить систему. В частности, в сценарии AD DS это 100 случайных операций ввода-вывода в секунду с шагом в 8 КБ, а всего 800 КБ в секунду при доступе к базе данных Jet. Кроме того, максимальная пропускная способность для шпинделя, выделенного для файлов журнала, имеет следующие ограничения. 300 последовательных операций ввода-вывода в секунду с шагом в 8 КБ, то есть всего 2400 КБ (2,4 МБ) в секунду.

Теперь, после анализа простой конфигурации, в следующей таблице показано, где происходит узкое место при изменении или добавлении компонентов в подсистеме хранения.

|Примечания|Анализ узкого места|Disk|Bus|Адаптер|Шина PCI|
|-|-|-|-|-|-|
|Это конфигурация контроллера домена после добавления второго диска. Конфигурация диска представляет собой узкое место в 800 КБ/с.|Добавить 1 диск (всего = 2)<br /><br />Ввод-вывод является случайным<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD|200 I/OS всего<br />800 КБ/с.| | | |
|После добавления 7 дисков конфигурация диска по-прежнему является узким местом в 3200 КБ/с.|**Добавление 7 дисков (всего = 8)**  <br /><br />Ввод-вывод является случайным<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD|800 операций ввода-вывода в секунду.<br />3200 КБ/с| | | |
|После изменения операций ввода-вывода на последовательный сетевой адаптер становится узким местом, так как он ограничен 1000 операций ввода/вывода.|Добавление 7 дисков (всего = 8)<br /><br />**Ввод-вывод является последовательным**<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD| | |2400 операции ввода-вывода в секунду могут быть прочитаны и записаны на диск, а контроллер — только 1000.| |
|После замены сетевого адаптера адаптером SCSI, поддерживающим 10 000 операций ввода-вывода в секунду, узкое место возвращается к конфигурации диска.|Добавление 7 дисков (всего = 8)<br /><br />Ввод-вывод является случайным<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD<br /><br />**Обновление адаптера SCSI (теперь поддерживает 10 000 операций ввода-вывода)**|800 операций ввода-вывода в секунду.<br />3 200 КБ/с| | | |
|После увеличения размера блока до 32 КБ эта шина становится узким местом, так как поддерживает только 20 МБ/с.|Добавление 7 дисков (всего = 8)<br /><br />Ввод-вывод является случайным<br /><br />**Размер блока 32 КБ**<br /><br />10 000 RPM HD| |800 операций ввода-вывода в секунду. 25 600 КБ/с (25 МБ/с) можно читать и записывать на диск.<br /><br />Шина поддерживает только 20 МБ/с| | |
|После обновления шины и добавления дополнительных дисков диск остается узким местом.|**Добавление 13 дисков (всего = 14)**<br /><br />Добавление второго адаптера SCSI с 14 дисками<br /><br />Ввод-вывод является случайным<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD<br /><br />**Обновление до 320 МБ/с SCSI-шины**|2800 операций ввода-вывода<br /><br />11 200 КБ/с (10,9 МБ/с)| | | |
|После изменения операций ввода-вывода на последовательный диск остается узким местом.|Добавление 13 дисков (всего = 14)<br /><br />Добавление второго адаптера SCSI с 14 дисками<br /><br />**Ввод-вывод является последовательным**<br /><br />Размер блока 4 КБ<br /><br />10 000 RPM HD<br /><br />Обновление до 320 МБ/с SCSI-шины|8 400 операций ввода-вывода<br /><br />33 600 Кб\с<br /><br />(32,8 Мб\с)| | | |
|После добавления более быстрых жестких дисков диск остается узким местом.|Добавление 13 дисков (всего = 14)<br /><br />Добавление второго адаптера SCSI с 14 дисками<br /><br />Ввод-вывод является последовательным<br /><br />Размер блока 4 КБ<br /><br />**15 000 RPM HD**<br /><br />Обновление до 320 МБ/с SCSI-шины|14 000 операций ввода-вывода<br /><br />56 000 КБ/с<br /><br />(54,7 МБ/с)| | | |
|После увеличения размера блока до 32 КБ шина PCI становится узким местом.|Добавление 13 дисков (всего = 14)<br /><br />Добавление второго адаптера SCSI с 14 дисками<br /><br />Ввод-вывод является последовательным<br /><br />**Размер блока 32 КБ**<br /><br />15 000 RPM HD<br /><br />Обновление до 320 МБ/с SCSI-шины| | | |14 000 операций ввода-вывода<br /><br />448 000 КБ/с<br /><br />(437 МБ/с) — ограничение на число операций чтения и записи для шпинделя.<br /><br />Шина PCI поддерживает теоретический максимум в 133 МБ/с (максимально эффективно это 75%).|

### <a name="introducing-raid"></a>Введение в RAID

Природа подсистемы хранения не изменяется при использовании контроллера массива. Он просто заменяет адаптер SCSI в вычислениях. Изменение — это стоимость чтения и записи данных на диск при использовании различных уровней массива (например, RAID 0, RAID 1 или RAID 5).

В RAID 0 данные распределяются по всем дискам в наборе RAID. Это означает, что во время операции чтения или записи часть данных извлекается из каждого диска или отправляется на него, увеличивая объем данных, которые могут транзитировать систему в течение одного периода времени. Таким же, в течение одной секунды на каждом шпинделе (с учетом дисков 10 000-RPM) можно выполнить 100 операций ввода-вывода. Общая сумма операций ввода-вывода, которую можно поддерживать, составляет N 100 раз в секунду для каждого шпинделя (в 100 * N операций ввода-вывода в секунду).

![Логический диск d:](media/capacity-planning-considerations-logical-d-drive.png)

В RAID 1 данные зеркально отражаются (дублируются) на паре дисков для обеспечения избыточности. Поэтому при выполнении операции чтения операций ввода-вывода данные можно считывать с обоих дисков в наборе. Это фактически делает емкость операций ввода-вывода на обоих дисках доступной во время операции чтения. Это значит, что операции записи не имеют преимуществ производительности в RAID 1. Это происходит потому, что одни и те же данные должны быть записаны на оба диска в целях обеспечения избыточности. Несмотря на то, что запись данных выполняется параллельно на обоих дисках, так как оба вращения заняты дублированием данных, операция записи ввода-вывода по сути предотвращает выполнение двух операций чтения. Таким путем, каждый ввод-вывод насчитывается два операции чтения операций ввода-вывода. Для определения общего числа выполняемых операций ввода-вывода можно создать формулу из этой информации:  

> *Чтение операций ввода-вывода* + &times;  =  *2 операций ввода-вывода*на*диск*  

Когда известно отношение операций чтения к записи и число шпинделей, можно получить следующее уравнение из приведенного выше уравнения, чтобы указать максимальное число операций ввода-вывода, поддерживаемое массивом.  

> *Максимальный IOPS на шпиндель* &times; 2 шпинделей &times; [( *%Читает* +  *%Записывае*) &divide; ( *%Читает* + 2 &times; *%Записывае*)] = *Всего IOPS*

RAID 1 + 0, ведет себя точно так же, как и RAID 1, в отношении расходов на чтение и запись. Однако теперь операции ввода-вывода распределяются по каждому зеркальному набору. Если командлет  

> *Максимальный IOPS на шпиндель* &times; 2 шпинделей &times; [( *%Читает* +  *%Записывае*) &divide; ( *%Читает* + 2 &times; *%Записывае*)] = *Всего IOPS*  

в наборе RAID 1, если кратность (*N*) наборов RAID 1 является полосковой, общее количество операций ввода-вывода, которое можно обработать, составляет &times; N операций ввода-вывода для каждого набора RAID 1:  

> *N* &times; {*Максимальный IOPS на шпиндель* &times; 2 шпинделей &times; [( *%Читает* +  *%пишет*) &divide; ( *%Читает* + 2 &times; *%пишет*)] } = *Всего IOPS*

В RAID 5, иногда называемой *n* + 1 RAID, данные распределяются по *n* шпинделям, а сведения о четности записываются на диск "+ 1". Однако при вводе-выводе на диск, а не в RAID 1 или 1 + 0, RAID 5 гораздо дороже. RAID 5 выполняет следующую процедуру при каждом отправке операций ввода-вывода в массив:

1. Чтение старых данных
1. Считать старую четность
1. Запись новых данных
1. Записать новое значение четности

Каждый запрос на запись ввода-вывода, переданный контроллеру массива операционной системой, требует выполнения четырех операций ввода-вывода, а запросы на запись, отправленные в течение одного операции чтения, выполняются четыре раза. Чтобы получить формулу для преобразования запросов ввода-вывода с точки зрения операционной системы на такие диски:  

> *Чтение* ввода-вывода + 4 &times; = операций*ввода* -вывода  

Аналогичным образом в наборе RAID 1, когда отношение операций чтения к записи и число шпинделей известно, следующее уравнение может быть получено из приведенного выше уравнения, чтобы определить максимальное количество операций ввода-вывода, поддерживаемое массивом (Обратите внимание, что общее число дисков не включает "диск" потерялся по четности):  

> *IOPS на шпиндель* &times; (*шпиндель* – 1) &times; [( *%Читает*  +  *%пишет*) &divide; ( *%Читает* + 4 &times; *%пишет*)] = *Всего IOPS*

### <a name="introducing-sans"></a>Знакомство с сетями San

Расширение сложности подсистемы хранения, когда сеть SAN вводится в среду, основные принципы не меняются, однако поведение ввода-вывода для всех систем, подключенных к сети хранения данных, необходимо учитывать. Одним из основных преимуществ использования сети SAN является дополнительный объем избыточности по сравнению с внутренним или внешним подключением к хранилищу, поэтому планирование емкости должно учитывать требования к отказоустойчивости. Кроме того, представлены дополнительные компоненты, которые необходимо вычислить. Разбиение сети SAN на компоненты компонентов:

- SCSI или Fibre Channel жесткий диск
- Объединительная плата канала единицы хранения
- Единицы хранения
- Модуль контроллера хранилища
- Коммутаторы SAN
- HBA (-ы)
- Шина PCI

При проектировании любой системы для избыточности включаются дополнительные компоненты, позволяющие удовлетворить потенциальные сбои. При планировании емкости очень важно исключить избыточный компонент из доступных ресурсов. Например, если в сети SAN есть два модуля контроллера, то все операции ввода-вывода одного модуля контроллера должны использоваться для общей пропускной способности ввода-вывода, доступной системе. Это происходит из-за того, что при сбое одного контроллера вся нагрузка на систему ввода-вывода, запрошенная всеми подключенными системами, должна быть обработана оставшимся контроллером. По мере того, как все планирование емкости выполняется для пиковых периодов использования, избыточные компоненты не должны быть разделены на доступные ресурсы, и запланированная Пиковая загрузка не должна превышать 80% нагрузки системы (для размещения пакетов или аномальных систем). поведение). Аналогичным образом избыточный коммутатор SAN, единица хранения и диски не следует разделять на вычисления операций ввода-вывода.

При анализе поведения жесткого диска SCSI или Fibre Channel метод анализа поведения, как описано выше, не изменяется. Хотя каждому протоколу присвоены определенные преимущества и недостатки, ограничивающий фактор для каждого диска является механическим ограничением жесткого диска.

Анализ канала в единице хранения выполняется точно так же, как вычисление ресурсов, доступных на шине SCSI, или пропускная способность (например, 20 МБ/с), разделенная на размер блока (например, 8 КБ). Если это отклонение от простого предыдущего примера, происходит Статистическая обработка нескольких каналов. Например, если имеется 6 каналов, каждая из которых поддерживает 20 МБ/с, то общее количество операций ввода-вывода и передачи данных, которые доступны, составляет 100 МБ/с (это верно, а это не 120 МБ/с). Опять же, отказоустойчивость — основной игрок в этом вычислении, в случае потери всего канала система остается только с 5 работающими каналами. Таким же, чтобы гарантировать соответствие ожиданиям производительности в случае сбоя, Общая пропускная способность для всех каналов хранения не должна превышать 100 МБ/с (это предполагает, что нагрузка и отказоустойчивость равномерно распределяются по всем каналам). Включение этого режима в профиль ввода-вывода зависит от поведения приложения. В случае Active Directory ввода-вывода в Jet это составит приблизительно 12 500 операций ввода-вывода в секунду (100 МБ/с &divide; 8 КБ на операцию ввода-вывода).

Далее, чтобы получить представление о пропускной способности, поддерживаемой каждым модулем, необходимо получить спецификации производителя для модулей контроллеров. В этом примере SAN имеет два модуля контроллера, которые поддерживают 7 500 операций ввода-вывода. Общая пропускная способность системы может составлять 15 000 операций ввода-вывода в секунду, если избыточность не требуется. При вычислении максимальной пропускной способности в случае сбоя ограничением является пропускная способность одного контроллера или 7 500 операций ввода-вывода в секунду. Это пороговое значение также находится ниже 12 500 операций ввода-вывода в секунду (при условии, что размер блока составляет 4 КБ), который может поддерживаться всеми каналами хранения, и, таким образом, в настоящее время является узким местом в анализе. По-прежнему для целей планирования, требуемый максимальный объем ввода-вывода будет 10 400.

Когда данные завершают модуль контроллера, он перераспределяет Fibre Channel с частотой 1 ГБ/с (или 1 гигабит в секунду). Чтобы сопоставить это с другими метриками, 1 ГБ/с преходят в 128 МБ/с (1 ГБ/с &divide; 8 бит/байт). Так как это превышает общую пропускную способность во всех каналах в единице хранения (100 МБ/с), это не повлияет на систему. Кроме того, так как это только один из двух каналов (дополнительное подключение Fibre Channel 1 ГБ/с для обеспечения избыточности), в случае сбоя одного подключения оставшееся подключение все еще имеет достаточную емкость для выполнения всех требуемых данных.

Направлять маршрут к серверу, данные, скорее всего, будут перенаправлены коммутатором SAN. Так как коммутатор SAN должен обработать входящий запрос ввода-вывода и перенаправить его на соответствующий порт, параметр будет ограничивать количество операций ввода-вывода, которые могут быть обработаны, однако для определения этого ограничения потребуется определить производителя. Например, если имеется два параметра и каждый коммутатор может обрабатывать 10 000 операций ввода-вывода в секунду, Общая пропускная способность будет составлять 20 000 операций ввода-вывода. Опять же, отказоустойчивость является проблемой, если один коммутатор завершается сбоем, Общая пропускная способность системы будет составлять 10 000 операций ввода-вывода в секунду. Так как желательно не превышать 80% использования в нормальных операциях, в качестве целевого используется не более 8000 операций ввода-вывода.

Наконец, адаптер HBA, установленный на сервере, также будет иметь ограничение на количество операций ввода-вывода, которое может быть обработано. Как правило, для обеспечения избыточности устанавливается второй HBA, но, как и при использовании коммутатора SAN, при вычислении максимального количества операций ввода-вывода, которые можно обработать, Общая пропускная способность в *N* &ndash; 1 HBA является максимальной масштабируемостью системы.

### <a name="caching-considerations"></a>Вопросы кэширования

Кэши — это один из компонентов, который может существенно повлиять на общую производительность в любой точке системы хранения. Подробный анализ алгоритмов кэширования выходит за рамки этой статьи. Однако некоторые основные инструкции о кэшировании в дисковых подсистемах стоит засветить:

- Кэширование обеспечивает улучшенную постоянную последовательную операцию ввода-вывода, так как она позволяет зарезервировать множество небольших операций записи в более крупные блоки ввода-вывода и выполнять промежуточное хранение в хранилище меньше, но более крупные размеры блоков. Это снизит общее число случайных операций ввода-вывода и общее число последовательных операций ввода-вывода, что обеспечивает дополнительную доступность ресурсов для других операций ввода-вывода.
- Кэширование не улучшает непрерывную пропускную способность ввода-вывода для подсистемы хранения. Он позволяет только записывать данные в буфер до тех пор, пока диски не будут доступны для фиксации данных. Когда все доступные операции ввода-вывода дисков в подсистеме хранения загружаются в течение длительных периодов, кэш в конечном итоге будет заполнен. Чтобы очистить кэш, достаточно времени между пакетами или дополнительными дисками, необходимо выделить достаточное количество операций ввода-вывода, чтобы разрешить сброс кэша.

  Большие кэши позволяют создавать больше данных только для буферизации. Это означает, что могут быть рассчитаны более длительные периоды насыщенности.

  В обычном подсистеме хранения операционная система будет улучшать производительность записи, так как данные должны быть записаны только в кэш. После того, как базовый носитель будет перезагружен с вводом-выводом, кэш заполнится, и запись производительности вернется на скорость диска.

- При кэшировании операций ввода-вывода чтения, сценарий, в котором кэш является наиболее выгодным, заключается в том, что данные хранятся на диске последовательно, а кэш может быть прочитан (в результате предполагается, что следующий сектор содержит данные, которые будут запрошены далее).
- Если чтение ввода-вывода происходит случайным образом, кэширование на контроллере диска вряд ли подает никаких улучшений в объем данных, которые можно считывать с диска. Любое расширение не является таковым, если размер кэша операционной системы или приложения превышает размер кэша на основе оборудования.

  В случае Active Directory кэш ограничивается только объемом ОЗУ.

### <a name="ssd-considerations"></a>Рекомендации по SSD

Твердотельные накопители — это совершенно разные животные, чем жесткие диски на основе шпинделей. Однако два ключевых критерия остаются: «Сколько операций в секунду может быть обработано?» и «какова задержка для этих операций ввода-вывода?» В сравнении с жесткими дисками на основе дисков SSDs может работать с большими объемами операций ввода-вывода и может иметь меньше задержек. В целом и на момент написания этой статьи, в то время как твердотельные накопители по-прежнему являются дорогостоящими в сравнении с затратами на гигабайты, они очень дешевы в отношении затрат на операции ввода-вывода и заслуживают значительных усилий с точки зрения производительности хранилища.

Замечания:

- Операции ввода-вывода в секунду и задержки очень актуальны для разработки производителей, и в некоторых случаях они могут быть более низкими, чем технологии на основе шпинделя. Коротко говоря, гораздо важнее проанализировать и проверить диск с характеристиками производителя по диску, а не думать о каких-либо обобщениях.
- Типы операций ввода-вывода могут иметь очень разные значения в зависимости от того, доступно ли оно для чтения или записи. AD DS службы, как правило, основаны на чтении, будут менее затронуты, чем другие сценарии приложений.
- «Write ендуранце» — это концепция, с которой в конечном итоге будут выставляться ячейки SSD. Разные производители имеют дело с другими способами. По крайней мере, в случае с диском базы данных, прочтенный профиль ввода-вывода позволяет довнплайинг важность этой проблемы, так как данные не сильно энергонезависимы.

### <a name="summary"></a>Сводка

Один из способов подумать о хранении — пиктуринг семьи. Представьте себе операции ввода-вывода носителя, на котором хранятся данные — основной сток для домашнего хозяйства. Если это возможно (например, в виде корней в канале) или ограничено (он свернут или слишком мал), все приемники в семье будут создавать резервные копии при использовании слишком большого объема воды (слишком много гостей). Это совершенно аналогично общей среде, где одна или несколько систем используют общее хранилище в сети SAN, NAS или iSCSI с одним и тем же базовым носителем. Для разрешения различных сценариев можно принять различные подходы.

- Для свернутой или недостаточной очистки требуется полная замена и исправление. Это будет аналогично добавлению в новое оборудование или повторному распространению систем с использованием общего хранилища во всей инфраструктуре.
- "Засоренный" канал обычно означает идентификацию одной или нескольких проблем с ошибками и их устранения. В сценарии хранилища это может быть резервное копирование на уровне хранилища или системы, синхронизация антивирусных проверок на всех серверах и синхронизированное программное обеспечение дефрагментации, выполняемое в периоды пиковой нагрузки.

В любой архитектурной модели несколько сток переводятся в основной сток. Если какая-либо из этих сток или точки соединения останавливается, то резервное копирование выполняется только на момент создания точки соединения. В сценарии с хранилищем это может быть перегруженный параметр (сценарий SAN/NAS/iSCSI), проблемы совместимости драйверов (неправильное сочетание встроенного по драйвера/адаптера HBA/Storport. sys), резервное копирование, антивирусная или дефрагментация. Чтобы определить, достаточно ли много места в хранилище, необходимо измерить размер операций ввода-вывода в секунду и объем операций ввода в нем. Для каждого соединения добавьте их вместе, чтобы обеспечить адекватный "Диаметр канала".

## <a name="appendix-d---discussion-on-storage-troubleshooting---environments-where-providing-at-least-as-much-ram-as-the-database-size-is-not-a-viable-option"></a>Приложение D. обсуждение вопросов, связанных с хранилищем — среды, в которых предоставляется по крайней мере как минимум ОЗУ, так как размер базы данных не является приемлемым вариантом.

Полезно понимать, почему эти рекомендации существуют, чтобы изменения в технологии хранения могли быть адаптированы. Эти рекомендации существуют по двум причинам. Первый — это изоляция операций ввода-вывода, поэтому проблемы производительности (т. е. разбиение на страницы) на уровне операционной системы не влияют на производительность базы данных и профилей ввода/вывода. Во-вторых, файлы журналов для AD DS (и большинства баз данных) являются последовательными, а жесткие диски на основе дисков и кэши имеют огромное преимущество при использовании с последовательным вводом-выводом по сравнению с более случайными схемами ввода-вывода операционной системы и почти исключительно случайные шаблоны ввода-вывода AD DS диска базы данных. Изолируя последовательный ввод-вывод на отдельный физический диск, можно увеличить пропускную способность. Проблема, представленная в современных вариантах хранения, заключается в том, что основные предположения, лежащие в основе этих рекомендаций, перестают быть верными. Во многих сценариях виртуализированного хранилища, таких как iSCSI, SAN, NAS и файлы образов виртуальных дисков, базовый носитель хранилища совместно используется на нескольких узлах, что позволяет полностью отменять как "изоляцию операций ввода-вывода", так и "оптимизацию последовательного ввода/вывода". В действительности эти сценарии добавляют дополнительный уровень сложности в том случае, если другие узлы, обращающиеся к общему носителю, могут снизить скорость реагирования на контроллер домена.

При планировании производительности хранилища необходимо учитывать три категории: состояние холодного кэша, состояние прогрева кэша, резервное копирование и восстановление. Состояние холодного кэша возникает в таких сценариях, как при первоначальной перезагрузке контроллера домена или при перезапуске службы Active Directory и отсутствии Active Directory данных в ОЗУ. Состояние горячего кэша — это место, в котором контроллер домена находится в стабильном состоянии, а база данных кэшируется. Важно отметить, что они будут иметь очень разные профили производительности и наличие достаточного объема оперативной памяти для кэширования всей базы данных, не помогая повысить производительность при холодном кэше. Можно подумать о проектировании производительности для этих двух сценариев со следующим аналогом, прогрев холодного кэша — «спринт» и запуск сервера с горячим кэшем — «Marathon».

Для сценария "холодный кэш" и "горячий кэш" вопрос о том, насколько быстро хранилище может перемещать данные с диска в память. Прогрев кэша — это ситуация, когда, со временем производительность повышается по мере того, как больше запросов повторно использует данные, скорость попадания в кэш увеличивается и частота необходимости перехода на диск уменьшается. В результате снижается негативное влияние на производительность при снижении дискового пространства. Любое снижение производительности является временным только при ожидании теплого и роста кэша до максимально допустимого размера, зависящего от системы. Диалоговое обсуждение можно упростить, чтобы быстро получить данные с диска, и это простая мера операций ввода-вывода, доступных для Active Directory, которые являются предметом доступности операций ввода-вывода в базовом хранилище. С точки зрения планирования, так как прогрев сценариев кэширования и резервного копирования и восстановления происходит на исключительных основаниях, обычно происходит в течение нескольких часов и является субъективным для нагрузки контроллера домена, общие рекомендации не существуют, за исключением того, что эти действия запланированы. для непиковых часов.

В большинстве случаев AD DS в основном считывает операции ввода-вывода, как правило, соотношение 90% READ/10% Write. Чтение ввода-вывода часто является узким местом для взаимодействия с пользователем, а также с чтением операций записи, что приводит к ухудшению производительности записи. Так как ввод-вывод в NTDS. dit является предварительно случайным, кэши обычно предоставляют минимальное преимущество чтения операций ввода-вывода, что значительно важнее для корректной настройки хранилища для профиля чтения.

Для нормальных эксплуатационных условий цель планирования хранилища сводится к уменьшению времени ожидания запроса от AD DS для возврата с диска. По сути это означает, что количество ожидающих и ожидающих операций ввода-вывода меньше или равно числу путей к диску. Это можно измерять различными способами. В сценарии наблюдения за производительностью Общая рекомендация заключается в том, что логический диск (\авг) с диском ( *\<с базой данных\>NTDS*) может быть менее 20 мс. Требуемый рабочий порог должен быть намного ниже, желательно, как можно ближе к скорости хранения, в диапазоне от 2 до 6 миллисекунд (. 002 до .006 секунды) в зависимости от типа хранилища.

Пример.

![Диаграмма задержки хранилища](media/capacity-planning-considerations-storage-latency.png)

Анализ диаграммы:

- **Зеленый овал слева —** Задержка остается постоянной в 10 мс. Нагрузка увеличивается с 800 до 2400 операций ввода-вывода в секунду. Это абсолютное основание того, насколько быстро запрос ввода-вывода может обрабатываться базовым хранилищем. Это зависит от особенностей решения хранилища.
- **Бургунди овал справа —** Пропускная способность остается плоской от выхода зеленого круга до конца коллекции данных, в то время как задержка продолжает увеличиваться. Это демонстрирует, что когда объемы запросов превышают физические ограничения базового хранилища, тем дольше запросы тратятся на очередь, ожидая отправки в подсистему хранения.

Применение этого набора знаний:

- **Влияние на пользователя, который запрашивает членство в большой группе:** Предположим, что для этого требуется чтение 1 МБ данных с диска, объем операций ввода-вывода и время, затрачиваемое на выполнение следующих действий:
  - Размер страниц базы данных Active Directory составляет 8 КБ. 
  - На диске необходимо считывать не менее 128 страниц. 
  - Если ничего не кэшируется, в пол (10 мс) это займет минимум 1,28 секунд, чтобы загрузить данные с диска, чтобы вернуть их клиенту. В 20 мс, где пропускная способность хранилища слишком велика с момента израсходован и является рекомендуемым максимумом, для получения данных с диска потребуется 2,5 секунд, чтобы возвратить их конечному пользователю.  
- **Когда скорость будет запрогревить кэш —** Предполагая, что нагрузка клиента будет максимально увеличить пропускную способность в этом примере хранилища, кэш будет тепло в 2400 операций &times; ввода-вывода в секунду 8 КБ на каждый ввод-вывод. Или примерно 20 МБ/с в секунду, Загрузка примерно 1 ГБ базы данных в ОЗУ каждые 53 секунд.

> [!NOTE]
> Это нормально для коротких периодов, чтобы наблюдать задержки, когда компоненты агрессивно считывают или записываются на диск, например при резервном копировании системы или при выполнении сборки мусора AD DS. Для поддержки этих периодических событий необходимо предоставить дополнительное головное пространство на основе вычислений. Цель — обеспечить достаточную пропускную способность для поддержки этих сценариев без влияния на нормальную функцию.

Как можно видеть, существует физический предел, основанный на проектировании хранилища для того, насколько быстро может быть тепло кэш. Содержимое кэша будет занимать входящий клиентский запрос к частоте, которую может предоставить базовый объем хранилища. Выполнение сценариев для "предварительной" загрузки кэша в часы пиковой нагрузки обеспечит конкуренцию на нагрузку, управляемую реальными клиентскими запросами. Это может негативно повлиять на доставку данных, которые требуются клиентам, поскольку, по своей структуре, создает конкуренцию для дефицитных дисковых ресурсов, так как искусственные попытки загорячий кэш загружают данные, не относящиеся к клиентам, которые подключаются к контроллеру домена.
