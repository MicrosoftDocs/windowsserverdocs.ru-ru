---
title: Настройка в программном обеспечении производительности шлюза HNV определенные сетей
description: HNV шлюза рекомендации по настройке производительности в сетях определенные программного обеспечения
ms.prod: windows-server-threshold
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: grcusanz; AnPaul
author: phstee
ms.date: 10/16/2017
ms.openlocfilehash: 217428b84a00b2e2231a15cb3878d0abcec1d9ad
ms.sourcegitcommit: 0d0b32c8986ba7db9536e0b8648d4ddf9b03e452
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/17/2019
ms.locfileid: "59827325"
---
# <a name="hnv-gateway-performance-tuning-in-software-defined-networks"></a>Настройка в программном обеспечении производительности шлюза HNV определенные сетей

Этот раздел содержит спецификации оборудования и рекомендаций по настройке для серверов, которые под управлением Hyper-V и размещены виртуальные машины шлюза Windows Server, а также параметры конфигурации для шлюза Windows Server виртуальных машин (ВМ) . Чтобы извлечь наилучшей производительности из виртуальных машин шлюза Windows Server, предполагается, что вы будете соблюдать следующие рекомендации.
В следующих разделах вы найдете требования к оборудованию и конфигурации для развертывания шлюза Windows Server.
1. Рекомендации по оборудованию для Hyper-V
2. Конфигурация узла Hyper-V
3. Конфигурация виртуальной Машины шлюза Windows Server

## <a name="hyper-v-hardware-recommendations"></a>Рекомендации по оборудованию для Hyper-V

Ниже представлена Минимальная рекомендованная аппаратная конфигурация для каждого сервера, на котором работает Windows Server 2016 и Hyper-V.

| Компонент сервера               | Спецификация                                                                                                                                                                                                                                                                   |
|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Центральный процессор (ЦП)  | Архитектура неоднородной памяти (NUMA) узлы: 2 <br> При наличии нескольких шлюз Windows Server виртуальных машин на узле, для наилучшей производительности каждого шлюза виртуальной Машины должна иметь полный доступ для одного узла NUMA. И она должна отличаться от узла NUMA, используемая адаптером физического узла. |
| Количество ядер на узел NUMA            | 2                                                                                                                                                                                                                                                                               |
| Hyper-Threading                | Отключено. Технология Hyper-Threading не повышает производительность шлюза Windows Server.                                                                                                                                                                                           |
| ОЗУ     | 48 ГБ                                                                                                                                                                                                                                                                           |
| Сетевые платы | Два сетевых адаптера 10 ГБ, производительность шлюза будет зависеть от скорости строки. Если скорость линии меньше 10 Гбит/с, показатели пропускной способности туннеля шлюза также стать недоступной на том же фактор.                                                                                          |

Убедитесь, что количество виртуальных процессоров, назначенных виртуальной машине шлюза Windows Server, не превышает количество процессоров на узле NUMA. Например, если на узле NUMA восемь ядер, количество виртуальных процессоров должно быть не больше восьми. Для наилучшей производительности следует 8. Чтобы узнать количество узлов NUMA и количество ядер на узел NUMA, запустите следующий сценарий Windows PowerShell на каждом узле Hyper-V.

```PowerShell
$nodes = [object[]] $(gwmi –Namespace root\virtualization\v2 -Class MSVM_NumaNode)
$cores = ($nodes | Measure-Object NumberOfProcessorCores -sum).Sum
$lps = ($nodes | Measure-Object NumberOfLogicalProcessors -sum).Sum


Write-Host "Number of NUMA Nodes: ", $nodes.count
Write-Host ("Total Number of Cores: ", $cores)
Write-Host ("Total Number of Logical Processors: ", $lps)
```

>[!Important]
> Выделение виртуальных процессоров за пределами узлов NUMA может отрицательно сказаться на производительности шлюза Windows Server. Использование нескольких виртуальных машин, виртуальные процессоры каждой из которых относятся к одному узлу NUMA, обеспечивает более высокую производительность, чем использование одной виртуальной машины, которой назначены все виртуальные процессоры.

Один шлюз виртуальной Машины с восемью виртуальными процессорами и по крайней мере 8 ГБ ОЗУ, рекомендуется при выборе числа виртуальных машин шлюза для установки на каждом узле Hyper-V, если каждый узел NUMA имеет восемь ядер. В этом случае одним узлом NUMA выделяется для хост-компьютере.

## <a name="hyper-v-host-configuration"></a>Конфигурация узла Hyper-V

Ниже представлена рекомендованная конфигурация для каждого сервера, на котором работает Windows Server 2016 и Hyper-V, котором должны работать виртуальные машины шлюза Windows Server. Эти инструкции по конфигурации включают примеры использования команд Windows PowerShell. В этих примерах вместо актуальных значений, которые вы должны предоставить при выполнении команды, используются заполнители. Например заполнителей имен сетевых адаптеров являются «NIC1? и «NIC2.? Когда вы выполняете команды, в которых используются эти заполнители, используйте фактические имена сетевых адаптеров на ваших серверах, иначе команда не будет выполнена.

>[!Note]
> Для запуска следующих команд Windows PowerShell вы должны быть членом группы администраторов.

| Элемент конфигурации                          | Настройки Windows Powershell                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Объединение внедренных коммутаторов                     | При создании виртуальный коммутатор с несколькими сетевыми адаптерами, автоматически включается switch embedded объединение для этих адаптеров. <br> ```New-VMSwitch -Name TeamedvSwitch -NetAdapterName "NIC 1","NIC 2"``` <br> С помощью SDN в Windows Server 2016 не поддерживает традиционные объединение через LBFO. Switch Embedded Teaming позволяет использовать тот же набор сетевых адаптеров для трафика виртуальной и трафика RDMA. Это не поддерживается с объединением Сетевых карт в зависимости от LBFO.                                                        |
| Управление прерываниями на физических сетевых адаптерах       | Используйте параметры по умолчанию. Чтобы проверить конфигурацию, можно использовать следующую команду Windows PowerShell: ```Get-NetAdapterAdvancedProperty```                                                                                                                                                                                                                                                                                                                                                                    |
| Размер буферов приема физических сетевых адаптеров       | Чтобы узнать, поддерживают ли физические сетевые адаптеры настройку этих параметров с помощью команды ```Get-NetAdapterAdvancedProperty```. Если они не поддерживают этот параметр, выходные данные команды не поддерживает свойство «приемных буферов.? Если сетевые адаптеры поддерживают этот параметр, можно воспользоваться следующей командой Windows PowerShell, чтобы настроить размер буферов приема. <br>```Set-NetAdapterAdvancedProperty “NIC1�? –DisplayName “Receive Buffers�? –DisplayValue 3000``` <br>                          |
| Размер буферов отправки физических сетевых адаптеров          | Чтобы узнать, поддерживают ли физические сетевые адаптеры настройку этих параметров с помощью команды ```Get-NetAdapterAdvancedProperty```. Если сетевые адаптеры не поддерживают этот параметр, выходные данные команды не поддерживает свойство «Send Buffers.? Если сетевые адаптеры поддерживают этот параметр, можно воспользоваться следующей командой Windows PowerShell, чтобы настроить размер буферов отправки. <br> ```Set-NetAdapterAdvancedProperty “NIC1�? –DisplayName “Transmit Buffers�? –DisplayValue 3000``` <br>                           |
| Масштабирование на стороне приема на физических сетевых адаптерах | Можно проверить наличие RSS включена с помощью команды Windows PowerShell Get-NetAdapterRss физических сетевых контроллеров. Чтобы включить и настроить RSS на сетевых адаптерах можно использовать следующие команды Windows PowerShell: <br> ```Enable-NetAdapterRss “NIC1�?,�?NIC2�?```<br> ```Set-NetAdapterRss “NIC1�?,�?NIC2�? –NumberOfReceiveQueues 16 -MaxProcessors``` <br> ПРИМЕЧАНИЕ. Если VMMQ или VMQ, RSS не обязательно включать на физических сетевых адаптеров. Его можно включить на виртуальных сетевых адаптеров узла |
| VMMQ                                        | Чтобы включить VMMQ для виртуальной Машины, выполните следующую команду: <br> ```Set-VmNetworkAdapter -VMName <gateway vm name>,-VrssEnabled $true -VmmqEnabled $true``` <br> ПРИМЕЧАНИЕ. Не все сетевые адаптеры поддерживают VMMQ. В настоящее время поддерживается в серии 45xxx Chelsio T5 и T6, Mellanox CX-3 и CX 4 и QLogic                                                                                                                                                                                                                                      |
| Очередь виртуальной машины в объединении сетевых карт | Вы можете включить VMQ вашей команды SET, с помощью следующей команды Windows PowerShell: <br>```Enable-NetAdapterVmq``` <br> ПРИМЕЧАНИЕ. Этот параметр должен быть включен только в том случае, если HW не поддерживает VMMQ. Если поддерживается, VMMQ должен быть включен для повышения производительности.                                                                                                                                                                                                                                                               |
>[!Note]
> VMQ и vRSS прийти рисунок только в том случае, когда нагрузка на виртуальной Машине достаточно высока и ЦП до максимума. Только после этого будет по крайней мере один процессор core max out. VMQ и vRSS затем может оказаться полезной, чтобы помочь распределить нагрузку между несколькими ядрами. Это не применимо для трафика IPsec, как трафик по протоколу IPsec сводится к единственному ядру.

## <a name="windows-server-gateway-vm-configuration"></a>Конфигурация виртуальной машины шлюза Windows Server

На обоих узлах Hyper-V можно настроить несколько виртуальных машин, настроенных в качестве шлюзов в шлюзе Windows Server. Можно воспользоваться диспетчером виртуального коммутатора, чтобы создать виртуальный коммутатор Hyper-V, привязанный к объединению сетевых карт на узле Hyper-V. Обратите внимание на то, что для оптимальной производительности следует развернуть один шлюз виртуальной Машины на узле Hyper-V.
Ниже представлена рекомендованная конфигурация для виртуальной машины шлюза Windows Server.

| Элемент конфигурации                 | Настройки Windows Powershell                                                                                                                                                                                                                                                                                                                                                               |
|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Память                             | 8 ГБ                                                                                                                                                                                                                                                                                                                                                                                           |
| Количество виртуальных сетевых адаптеров | Использует тремя сетевыми адаптерами с определенными следующие: 1 для управления, используется управляющей операционной системе, одна внешняя сетевая карта, предоставляющий доступ к внешним сетям, 1, внутренний для предоставления доступа только ко внутренним сетям.                                                                                                                                                            |
| Receive Side Scaling (RSS)         | Можно сохранить стандартные параметры RSS для сетевого адаптера управления. Ниже приведен пример конфигурации для виртуальной машины с восемью виртуальными процессорами. Для внешнего и внутреннего сетевых адаптеров вы можете включить Baseprocnumber значение 0, а также параметра maxrssprocessors — значение 8, с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterRss “Internal�?,�?External�? –BaseProcNumber 0 –MaxProcessorNumber 8``` <br> |
| Буфер на стороне отправки                   | Можно сохранить стандартные параметры буфера на стороне отправки для сетевого адаптера управления. Для внутренних и внешних сетевых адаптеров можно настроить буфера на стороне отправки с 32 МБ ОЗУ с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty “Internal�?,�?External�? –DisplayName “Send Buffer Size�? –DisplayValue “32MB�?``` <br>                                                       |
| Буфер на стороне приема                | Можно сохранить стандартные параметры буфера на стороне приема для сетевого адаптера управления. Для внутренних и внешних сетевых адаптеров можно настроить буфера на стороне приема с 16 МБ оперативной памяти с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty “Internal�?,�?External�? –DisplayName “Receive Buffer Size�? –DisplayValue “16MB�?``` <br>                                            |
| Оптимизация переадресации               | Можно сохранить стандартные параметры оптимизации переадресации для сетевого адаптера управления Для внутренних и внешних сетевых адаптеров вы можете включить оптимизацию переадресации с помощью следующей команды Windows PowerShell: <br> ```Set-NetAdapterAdvancedProperty “Internal�?,�?External�? –DisplayName “Forward Optimization�? –DisplayValue “1�?``` <br>                                                                      |
